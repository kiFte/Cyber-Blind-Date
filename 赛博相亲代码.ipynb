{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import re # 正则表达式模块\n",
    "from IPython.display import HTML\n",
    "from haversine import haversine, Unit \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import altair as alt\n",
    "import warnings\n",
    "from hanlp_restful import HanLPClient\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2 # 导入psycopg2库，用于连接PostgreSQL数据库\n",
    "import jieba\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from sqlalchemy.sql import text  # 添加此行导入 text\n",
    "import json\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "# 忽略 FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入csv数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定列名和对应的数据类型\n",
    "dtype_mapping = {\n",
    "    '2.小红书号id': str,\n",
    "    '3.B站昵称': str,\n",
    "    '4.抖音号': str,\n",
    "    '5.邮箱': str,\n",
    "    '6.公示用昵称': str,\n",
    "    '7.小黑盒id': str,\n",
    "    '11.身高（单位cm）': str,  # 暂时改为字符串，后续检查异常值\n",
    "    '42.TA的月均收入_填空1': str,  # 暂时改为字符串\n",
    "    '42.TA的月均收入_填空2': str,   # 暂时改为字符串\n",
    "    '43.TA的个人总资产_填空1': str,  # 暂时改为字符串\n",
    "    '43.TA的个人总资产_填空2': str,  # 暂时改为字符串\n",
    "}\n",
    "# 读取 CSV 文件\n",
    "file_path = r\"D:\\code\\相亲问卷数据存储\\最后一场.csv\"\n",
    "df = pd.read_csv(file_path, dtype=dtype_mapping)\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础清理：去除所有字符串数据的前后空格\n",
    "df1 = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# 删除指定列：UA、Referrer、智能清洗数据无效概率、清洗数据结果\n",
    "columns_to_drop = [\"UA\", \"Referrer\", \"智能清洗数据无效概率\", \"清洗数据结果\", \"语言\", \"自定义字段\", '地理位置国家和地区', '地理位置省']\n",
    "df1 = df1.drop(columns=columns_to_drop, errors=\"ignore\")  # 使用 errors=\"ignore\" 避免列不存在时报错\n",
    "\n",
    "# 删除列名开头为 \"Unnamed：\" 的列\n",
    "df1 = df1.loc[:, ~df1.columns.str.contains(r'^Unnamed')]\n",
    "\n",
    "# 去除列名前面的数字和点（如 \"1.列名\" -> \"列名\"）\n",
    "df1.columns = df1.columns.str.replace(r'^\\d+\\.', '', regex=True)\n",
    "\n",
    "# 清理地址数据：去掉常驻地址字段中的 `/` 符号\n",
    "df1['常驻地址(经度，纬度)'] = df1['常驻地址(经度，纬度)'].str.replace('/', '', regex=False)\n",
    "df1['家乡地址(经度，纬度)'] = df1['家乡地址(经度，纬度)'].str.replace('/', '', regex=False)\n",
    "\n",
    "# 将类型转换为数值，遇到错误设置为 NaN\n",
    "df1['身高（单位cm）'] = pd.to_numeric(df1['身高（单位cm）'], errors='coerce')\n",
    "df1['TA的月均收入（单位：元）_填空1'] = pd.to_numeric(df1['TA的月均收入（单位：元）_填空1'], errors='coerce')\n",
    "df1['TA的月均收入（单位：元）_填空2'] = pd.to_numeric(df1['TA的月均收入（单位：元）_填空2'], errors='coerce')\n",
    "df1['TA的个人总资产（单位：万元）_填空1'] = pd.to_numeric(df1['TA的个人总资产（单位：万元）_填空1'], errors='coerce')\n",
    "df1['TA的个人总资产（单位：万元）_填空2'] = pd.to_numeric(df1['TA的个人总资产（单位：万元）_填空2'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.info())\n",
    "print(df1.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建更多信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more = df1.copy()\n",
    "print(df_more.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基础信息处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出生年份是出生日期的最后四个字符\n",
    "df_more['出生年份'] = df_more['出生日期'].str[:4].astype(int)\n",
    "print(df_more['出生年份'].head(2))\n",
    "\n",
    "# BMI 按照公式计算\n",
    "df_more['BMI'] = round(df_more['体重（单位kg）'] / (df_more['身高（单位cm）'] / 100) ** 2,2)\n",
    "print(df_more['BMI'].head(2))\n",
    "\n",
    "# 确保地址列数据格式正确，分离为经度和纬度\n",
    "df_more[['常驻经度', '常驻纬度']] = df_more['常驻地址(经度，纬度)'].str.split('，', expand=True).astype(float)\n",
    "df_more[['家乡经度', '家乡纬度']] = df_more['家乡地址(经度，纬度)'].str.split('，', expand=True).astype(float)\n",
    "# 生产字段(纬度,经度)，用于计算距离\n",
    "df_more['常驻坐标'] = list(zip(df_more['常驻纬度'], df_more['常驻经度']))\n",
    "df_more['家乡坐标'] = list(zip(df_more['家乡纬度'], df_more['家乡经度']))\n",
    "# 身高中的异常值处理，将\"-\"替换为\"0\"，并将条件整列的数据类型转换为int\n",
    "df_more['身高（单位cm）'] = df_more['身高（单位cm）'].replace('-', 0).astype(float)\n",
    "df_more['TA的身高（单位cm）:底线-最低'] = df_more['TA的身高（单位cm）:底线-最低'].replace('-', 0).astype(int)\n",
    "df_more['TA的身高（单位cm）:底线-最高'] = df_more['TA的身高（单位cm）:底线-最高'].replace('-', 0).astype(int)\n",
    "df_more['TA的身高（单位cm）:加分项-最低'] = df_more['TA的身高（单位cm）:加分项-最低'].replace('-', 0).astype(int)\n",
    "df_more['TA的身高（单位cm）:加分项-最高'] = df_more['TA的身高（单位cm）:加分项-最高'].replace('-', 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除常驻地址(经度，纬度)\n",
    "df_more1 = df_more.drop(columns=[ '常驻经度', '常驻纬度', '家乡经度', '家乡纬度', '常驻地址', '家乡地址'])\n",
    "# 删除体重\n",
    "df_more1 = df_more1.drop(columns=['体重（单位kg）', '出生日期'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生产MBTI字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通用函数，用于根据输入字段解析MBTI类型\n",
    "def 提取MBTI字段(字段: str, 映射: dict) -> str:\n",
    "    匹配结果 = re.search(r'【(\\d)】', 字段)\n",
    "    if 匹配结果:\n",
    "        数字 = 匹配结果.group(1)\n",
    "        return 映射.get(数字, 'O')  # 如果匹配到数字但是不在映射中，输出报错“MBTI生成阶段错误：未知的数字”\n",
    "    return 'O'  # 如果没有匹配到数字，返回 'O'\n",
    "\n",
    "# 主函数，整合MBTI字段\n",
    "def 生产MBTI字段(生活态度: str, 感知方式: str, 判断方式: str, 生活方式: str) -> str:\n",
    "    映射表 = {\n",
    "        \"生活态度\": {\"1\": \"I\", \"2\": \"E\"},\n",
    "        \"感知方式\": {\"1\": \"S\", \"2\": \"N\"},\n",
    "        \"判断方式\": {\"1\": \"T\", \"2\": \"F\"},\n",
    "        \"生活方式\": {\"1\": \"J\", \"2\": \"P\"},\n",
    "    }\n",
    "    # 按照映射表生成MBTI字段\n",
    "    MBTI = (\n",
    "        提取MBTI字段(生活态度, 映射表[\"生活态度\"])\n",
    "        + 提取MBTI字段(感知方式, 映射表[\"感知方式\"])\n",
    "        + 提取MBTI字段(判断方式, 映射表[\"判断方式\"])\n",
    "        + 提取MBTI字段(生活方式, 映射表[\"生活方式\"])\n",
    "    )\n",
    "    return MBTI\n",
    "\n",
    "# DataFrame 中应用\n",
    "df_more1['MBTI'] = df_more1.apply(\n",
    "    lambda x: 生产MBTI字段(x['MBTI:生活态度【1：I】or【2：E】'], \n",
    "                          x['MBTI:感知方式【1：S】or【2：N】'],\n",
    "                          x['MBTI:判断方式【1：T】or【2：F】'],\n",
    "                          x['MBTI:生活方式【1：J】or【2：P】']\n",
    "    ),axis=1\n",
    ")\n",
    "df_more1['TA的MBTI'] = df_more1.apply(\n",
    "    lambda x: 生产MBTI字段(x['TA的MBTI（仅作为加分项）:生活态度【1：I】or【2：E】'],\n",
    "                          x['TA的MBTI（仅作为加分项）:感知方式【1：S】or【2：N】'],\n",
    "                          x['TA的MBTI（仅作为加分项）:判断方式【1：T】or【2：F】'],\n",
    "                          x['TA的MBTI（仅作为加分项）:生活方式【1：J】or【2：P】']\n",
    "    ),axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# 查看前5行的MBTI和生活态度、感知方式、判断方式、生活方式字段\n",
    "display(df_more1[['MBTI', 'MBTI:生活态度【1：I】or【2：E】', 'MBTI:感知方式【1：S】or【2：N】', 'MBTI:判断方式【1：T】or【2：F】','MBTI:生活方式【1：J】or【2：P】']].head(5))\n",
    "display(df_more1[['TA的MBTI', 'TA的MBTI（仅作为加分项）:生活态度【1：I】or【2：E】', 'TA的MBTI（仅作为加分项）:感知方式【1：S】or【2：N】', 'TA的MBTI（仅作为加分项）:判断方式【1：T】or【2：F】','TA的MBTI（仅作为加分项）:生活方式【1：J】or【2：P】']].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除MBTI:生活态度【1：I】or【2：E】、MBTI:感知方式【1：S】or【2：N】、MBTI:判断方式【1：T】or【2：F】、MBTI:生活方式【1：J】or【2：P】\n",
    "df_more2 =df_more1.drop(columns=['MBTI:生活态度【1：I】or【2：E】', 'MBTI:感知方式【1：S】or【2：N】', 'MBTI:判断方式【1：T】or【2：F】', 'MBTI:生活方式【1：J】or【2：P】'])\n",
    "df_more2 =df_more2.drop(columns=['TA的MBTI（仅作为加分项）:生活态度【1：I】or【2：E】', 'TA的MBTI（仅作为加分项）:感知方式【1：S】or【2：N】', 'TA的MBTI（仅作为加分项）:判断方式【1：T】or【2：F】', 'TA的MBTI（仅作为加分项）:生活方式【1：J】or【2：P】'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_more2.info())\n",
    "print(df_more2.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成识别用id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入一个 DataFrame，返回一个 DataFrame\n",
    "def 生成识别用id字段(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 将字段重命名\n",
    "    df.rename(columns={'你是在哪个平台收到这份问卷的？': '填写平台'}, inplace=True)\n",
    "    \n",
    "    # 创建空的 '识别用id' 列\n",
    "    df['识别用id'] = None\n",
    "\n",
    "    # 确保涉及拼接的字段为字符串类型，并处理 NaN 值\n",
    "    df['公示用昵称'] = df['公示用昵称'].fillna('').astype(str)\n",
    "    df['邮箱'] = df['邮箱'].fillna('').astype(str)\n",
    "    df['小红书号id'] = df['小红书号id'].fillna('').astype(str)\n",
    "    df['B站昵称'] = df['B站昵称'].fillna('').astype(str)\n",
    "    df['抖音号'] = df['抖音号'].fillna('').astype(str)\n",
    "    df['小黑盒id'] = df['小黑盒id'].fillna('').astype(str)\n",
    "\n",
    "    # 使用矢量化逻辑处理每个平台的情况\n",
    "    df.loc[df['填写平台'] == 'A.小红书', '识别用id'] = df['小红书号id']\n",
    "    df.loc[df['填写平台'] == 'B.B站', '识别用id'] = df['B站昵称']\n",
    "    df.loc[df['填写平台'] == 'C.抖音', '识别用id'] = df['抖音号']\n",
    "    df.loc[df['填写平台'] == 'D.西瓜视频', '识别用id'] = df['抖音号']\n",
    "    df.loc[df['填写平台'] == 'E.博客（需填写邮箱）', '识别用id'] = df['公示用昵称'] + '-邮箱前四位为' + df['邮箱'].str[:4]\n",
    "    df.loc[df['填写平台'] == 'F.小黑盒', '识别用id'] = df['小黑盒id']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 调用函数生成识别用 id 字段\n",
    "df_more3 = 生成识别用id字段(df_more2)\n",
    "# 别在这删除平台字段，不然后面筛除id会很麻烦\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_more3.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成地级市"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库连接信息\n",
    "user = 'postgres'\n",
    "password = 'root2'  # 替换为你的数据库密码\n",
    "host = 'localhost'\n",
    "port = 5655 # 替换为你的数据库端口号\n",
    "database = 'postgres'  # 替换为你的数据库名称\n",
    "schema = 'public'  # 替换为你的架构名称\n",
    "table_name = 'city_20250105'  # 替换为你的表名称\n",
    "\n",
    "# 创建 PostgreSQL 数据库连接\n",
    "try:\n",
    "    # 直接创建连接\n",
    "    conn = psycopg2.connect(database=database, user=user, password=password, host=host, port=port)\n",
    "    conn.autocommit = True  # 自动提交模式\n",
    "\n",
    "    # 设置搜索路径\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(f'SET search_path TO {schema};')\n",
    "    print(\"连接成功，并已设置搜索路径\")\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"数据库连接失败: {e}\")\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n",
    "        \n",
    "# 使用 SQLAlchemy 创建引擎\n",
    "try:\n",
    "    engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}?options=-csearch_path={schema}')\n",
    "    print(\"SQLAlchemy 引擎创建成功\")\n",
    "except Exception as e:\n",
    "    print(f\"引擎创建失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义查询函数\n",
    "def 根据经纬度取城市(row,经纬度字段:str = '活动地址(经度，纬度)') -> str:\n",
    "    # 预处理：将中文逗号“，”替换为英文逗号“,”\n",
    "    coords = row[经纬度字段].replace('，', ',').strip()\n",
    "    \n",
    "    # 判断是否为空或只有逗号\n",
    "    if not coords or coords == ',':\n",
    "        return None  # 无效数据直接返回 None\n",
    "\n",
    "    # 构建 SQL 查询语句\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        name\n",
    "    FROM\n",
    "        {table_name}\n",
    "    WHERE\n",
    "        deep = 1\n",
    "        AND ST_Intersects(\n",
    "            polygon,\n",
    "            ST_SetSRID(ST_Point({coords}), 0) -- 保持SRID一致\n",
    "        )\n",
    "    ORDER BY id\n",
    "    LIMIT 1;\n",
    "    \"\"\"\n",
    "\n",
    "    # 执行查询\n",
    "    try:\n",
    "        result = pd.read_sql(query, engine)  # 查询数据库\n",
    "        if not result.empty:\n",
    "            return result.iloc[0]['name']  # 返回第一行的 name 字段\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"查询失败2: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more4 = df_more3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 apply 方法调用函数时无需显式传递 row 参数\n",
    "df_more4['常驻市'] = df_more4.apply(lambda row: 根据经纬度取城市(row, \"常驻地址(经度，纬度)\"), axis=1)\n",
    "df_more4['家乡市'] = df_more4.apply(lambda row: 根据经纬度取城市(row, \"家乡地址(经度，纬度)\"), axis=1)\n",
    "\n",
    "# 显示结果\n",
    "display(df_more4[['识别用id', '常驻市', '家乡市', '地理位置市', '常驻地址(经度，纬度)']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除常驻地址(经度，纬度)\n",
    "df_more4 = df_more4.drop(columns=[ '地理位置市', '常驻地址(经度，纬度)', '家乡地址(经度，纬度)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 简化多选题匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more5 = df_more4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 提取英文字母函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取符合规则的字母组合并拼接成字符串\n",
    "def extract_and_concat_letters(row):\n",
    "    # 提取每列中的有效标签\n",
    "    valid_matches = []\n",
    "    pattern = r'([A-Z]{1,2}\\.)'  # 匹配规则\n",
    "\n",
    "    for col in row.dropna():  # 遍历非空列\n",
    "        col_text = str(col)  # 转为字符串\n",
    "        matches = re.findall(pattern, col_text)  # 提取匹配项\n",
    "        valid_matches.extend(matches)  # 累积匹配结果\n",
    "\n",
    "    # 拼接所有提取的标签\n",
    "    return ''.join(valid_matches).strip()  # 移除首尾空格\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出列名包含 \"兴趣爱好:\" 的列\n",
    "包含兴趣爱好的列 = df_more5.columns[df_more5.columns.str.contains(\"兴趣爱好:\")]\n",
    "df_more5['兴趣爱好'] = df_more5[包含兴趣爱好的列].apply(extract_and_concat_letters, axis=1)\n",
    "\n",
    "# 筛选出列名包含 \"勾选与你相符的描述:\" 的列\n",
    "包含相符描述的列 = df_more5.columns[df_more5.columns.str.contains(\"勾选与你相符的描述:\")]\n",
    "df_more5['相符描述'] = df_more5[包含相符描述的列].apply(extract_and_concat_letters, axis=1)\n",
    "\n",
    "# 筛选出列名包含 \"勾选TA需要符合的特点（仅作为加分项）:\" 的列\n",
    "包含TA相符描述的列 = df_more5.columns[df_more5.columns.str.contains(\"勾选与TA相符的描述（仅作为加分项）\")]\n",
    "df_more5['TA的相符描述'] = df_more5[包含TA相符描述的列].apply(extract_and_concat_letters, axis=1)\n",
    "\n",
    "# 筛选出列名包含 \"TA的学历（底线）:\" 的列\n",
    "包含学历底线 = df_more5.columns[df_more5.columns.str.contains(\"TA的学历（底线）:\")]\n",
    "df_more5['TA的学历（底线）'] = df_more5[包含学历底线].apply(extract_and_concat_letters, axis=1)\n",
    "\n",
    "# 筛选出列名包含 \"TA的学历（加分项）:\" 的列\n",
    "包含学历加分项 = df_more5.columns[df_more5.columns.str.contains(\"TA的学历（加分项）:\")]\n",
    "df_more5['TA的学历（加分项）'] = df_more5[包含学历加分项].apply(extract_and_concat_letters, axis=1)\n",
    "\n",
    "# 筛选出列名包含 \"TA的工作性质（仅作为加分项）:\" 的列\n",
    "包含工作性质 = df_more5.columns[df_more5.columns.str.contains(\"TA的工作性质（仅作为加分项）:\")]\n",
    "df_more5['TA的工作性质（仅作为加分项）'] = df_more5[包含工作性质].apply(extract_and_concat_letters, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 提取字母对应的含义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''提取兴趣爱好\n",
    "1. 查看所有包含兴趣爱好的列的列名\n",
    "    由于列名按照顺序的，所以可以根据列的数量创造一个字典，并给设置字母\n",
    "2. 提取每个列名中冒号后面的文本，将其和字母按顺序映射起来。这里可能需要定义一个函数，用于提取冒号后面的文本。\n",
    "'''\n",
    "# 查看所有包含兴趣爱好的列的列名\n",
    "print(包含兴趣爱好的列)\n",
    "\n",
    "def 数字转字母序列(n: int) -> str:\n",
    "    \"\"\"\n",
    "    将数字转换为字母序列（A, B, ..., Z, AA, AB, ..., AZ, BA, BB, ...）\n",
    "    \"\"\"\n",
    "    结果 = \"\"\n",
    "    while n > 0:\n",
    "        n -= 1  # 因为 A 对应 0，B 对应 1，...\n",
    "        结果 = chr(65 + n % 26) + 结果  # 65 是 'A' 的 ASCII 码\n",
    "        n = n // 26\n",
    "    return 结果\n",
    "\n",
    "def 提取标签字典(list: list[str]) -> Dict[str, str]:\n",
    "    pattern = r':(.*)'  # 匹配规则\n",
    "    结果字典 = {}  # 创建一个空字典\n",
    "    \n",
    "    # 遍历列表中每一个元素，生成字母序列作为键，并提取出冒号后面的文本加入字典\n",
    "    for i, col in enumerate(list):\n",
    "        match = re.search(pattern, col)\n",
    "        if match:  # 如果匹配成功\n",
    "            键 = 数字转字母序列(i + 1)  # 生成字母序列（从 A 开始）\n",
    "            结果字典[键] = match.group(1).strip()  # 提取内容并去除前后空格\n",
    "    \n",
    "    return 结果字典\n",
    "\n",
    "兴趣爱好标签 = 提取标签字典(包含兴趣爱好的列)\n",
    "display(兴趣爱好标签)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除包含兴趣爱好的列、包含相符描述的列、包含TA相符描述的列、包含学历底线、包含学历加分项、包含工作性质\n",
    "df_more6 = df_more5.drop(columns=包含兴趣爱好的列)\n",
    "df_more6 = df_more6.drop(columns=包含相符描述的列)\n",
    "df_more6 = df_more6.drop(columns=包含TA相符描述的列)\n",
    "df_more6 = df_more6.drop(columns=包含学历底线)\n",
    "df_more6 = df_more6.drop(columns=包含学历加分项)\n",
    "df_more7 = df_more6.drop(columns=包含工作性质)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提取赋权排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 提取排序字典(row):\n",
    "    # 从列名中提取冒号后的字段名，并将这些字段与对应的行值映射成字典。\n",
    "    pattern = r\":(.*)\"\n",
    "    result = {}\n",
    "\n",
    "    for col in row.index:  # 遍历列名\n",
    "        match = re.search(pattern, col)\n",
    "        if match:\n",
    "            key = match.group(1).strip()\n",
    "            value = row[col]\n",
    "            result[key] = value\n",
    "    # 将字典按值升序排序\n",
    "    sorted_result = dict(sorted(result.items(), key=lambda item: item[1]))\n",
    "    \n",
    "    return sorted_result  # 返回排序后的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more8 = df_more7.copy()\n",
    "# 提取列名\n",
    "包含看重的点进行排序的列 = df_more7.columns[df_more7.columns.str.contains(\"给你看重的点进行排序:\")]\n",
    "df_more8['赋权排序'] = df_more7[包含看重的点进行排序的列].apply(提取排序字典, axis=1)\n",
    "# 把所有 numpy 类型值转换为 Python 原生类型\n",
    "def safe_dict_to_json(d):\n",
    "    clean_dict = {k: int(v) if isinstance(v, (np.integer, np.int64)) else v for k, v in d.items()}\n",
    "    return json.dumps(clean_dict, ensure_ascii=False)\n",
    "\n",
    "df_more8['赋权排序_json'] = df_more8['赋权排序'].apply(safe_dict_to_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more9 = df_more8.drop(columns=包含看重的点进行排序的列)\n",
    "df_more9 = df_more9.drop(columns=['赋权排序'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more_end = df_more9.copy()\n",
    "df_more_end.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 筛除无效问卷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_more_end.copy()\n",
    "print(df_clean.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基础原则：不应当规定底线和加分项两个值之间哪个更大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''年龄大于等于 18 岁 \n",
    "提取当前的年份，\n",
    "与出生年份进行计算，得到年龄，\n",
    "筛选出大于等于 18 岁的数据\n",
    "'''\n",
    "# 提取当前年份\n",
    "current_year = pd.Timestamp.now().year\n",
    "# 计算年龄\n",
    "df_clean0 = df_clean.copy()\n",
    "df_clean0['年龄'] = current_year - df_clean0['出生年份']\n",
    "# 筛选年龄大于等于 18 岁的数据\n",
    "df_clean0 = df_clean0[df_clean0['年龄'] >= 18]\n",
    "print(f'0：{df_clean0.shape[0]}')\n",
    "# 删除年龄字段\n",
    "df_clean0 = df_clean0.drop(columns=['年龄'])\n",
    "\n",
    "# 答题时长大于 2 分钟的数据\n",
    "df_clean1 = df_clean0[df_clean0[\"答题时长\"] > 120]\n",
    "print(f'1：{df_clean1.shape[0]}')\n",
    "\n",
    "# IP 重复的数据，仅保留结束答题时间最新的一条\n",
    "df_clean2 = df_clean1.sort_values(by=\"结束答题时间\")\n",
    "df_clean2 = df_clean2.drop_duplicates(subset=[\"IP\"], keep=\"last\")\n",
    "# 平台&识别用id 同时重复的数据，仅保留结束答题时间最新的一条\n",
    "df_clean2 = df_clean2.sort_values(by=\"结束答题时间\")\n",
    "df_clean2 = df_clean2.drop_duplicates(subset=[\"填写平台\", \"识别用id\"], keep=\"last\")\n",
    "print(f'2：{df_clean2.shape[0]}')\n",
    "\n",
    "# BMI最低底线数字应小于等于30，最高底线数字应大于等于10\n",
    "df_clean3 = df_clean2[df_clean2[\"TA的身体质量指数BMI:底线-最低\"] <= 30]\n",
    "df_clean3 = df_clean2[df_clean2[\"TA的身体质量指数BMI:底线-最高\"] >= 10]\n",
    "print(f'3：{df_clean3.shape[0]}')\n",
    "\n",
    "\"\"\" \n",
    "个人总资产应当小于等于家庭总资产\n",
    "TA的个人总资产和家庭总资产之间不需要筛选，有很多人不在意对方的家庭总资产，在家庭总资产方面要求奇低\n",
    "\"\"\"\n",
    "df_clean4 = df_clean3[df_clean3[\"个人总资产（单位：万元）\"] <= df_clean3[\"家庭总资产（单位：万元）\"]]\n",
    "print(f'4：{df_clean4.shape[0]}')\n",
    "\n",
    "# 距离的底线应当大于0km\n",
    "df_clean5 = df_clean4[df_clean4[\"TA的地址与你的距离（单位km）:常驻地址相距-底线\"] > 0]\n",
    "df_clean5 = df_clean5[df_clean5[\"TA的地址与你的距离（单位km）:家乡地址相距-底线\"] > 0]\n",
    "print(f'5：{df_clean5.shape[0]}')\n",
    "\n",
    "# 收入和资产值不为空值\n",
    "df_clean6 = df_clean5.copy()\n",
    "df_clean6 = df_clean5[df_clean5[\"TA的月均收入（单位：元）_填空1\"].notnull()]\n",
    "df_clean6 = df_clean6[df_clean6[\"TA的月均收入（单位：元）_填空2\"].notnull()]\n",
    "df_clean6 = df_clean6[df_clean6[\"TA的个人总资产（单位：万元）_填空1\"].notnull()]\n",
    "df_clean6 = df_clean6[df_clean6[\"TA的个人总资产（单位：万元）_填空2\"].notnull()]\n",
    "print(f'6：{df_clean6.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 根据平台规则过滤id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例校验规则\n",
    "field_rules = {\n",
    "    \"小红书号id\": r'^[A-Za-z0-9_]+$',  # 只允许字母、数字和下划线\n",
    "    \"B站昵称\": r'.*',                 # 无限制\n",
    "    \"抖音号\": r'^\\d+$',               # 只允许数字\n",
    "    \"邮箱\": r'^[^@]+@[^@]+\\.[^@]+$',  # 邮箱格式校验\n",
    "    \"小黑盒id\": r'^\\d+$'              # 只允许数字\n",
    "}\n",
    "\n",
    "# 复制数据\n",
    "df_clean7 = df_clean6.copy()\n",
    "print(f'7: {df_clean7.shape[0]}')\n",
    "df_clean8 = df_clean7.copy()\n",
    "\n",
    "# 根据字段名和正则表达式进行校验\n",
    "for field, regex in field_rules.items():\n",
    "    if field in df_clean8.columns:  # 确保字段存在\n",
    "        # 重新将空字符串转换为 NaN\n",
    "        df_clean8[field] = df_clean8[field].replace(\"\", pd.NA)\n",
    "        # 校验字段，非空时必须匹配正则表达式\n",
    "        df_clean8 = df_clean8[df_clean8[field].str.fullmatch(regex, na=True)] # na=True 表示 NaN 值会被视为匹配\n",
    "\n",
    "print(f'校验后的数据行数: {df_clean8.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean9 = df_clean8.drop(columns=['小红书号id', 'B站昵称', '抖音号', '公示用昵称', '小黑盒id'])\n",
    "df_clean9 = df_clean8.drop(columns=['开始答题时间'])\n",
    "df_clean9.info()\n",
    "print(df_clean7.shape[0])\n",
    "print(df_clean9.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调整范围大小项顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean10 = df_clean9.copy()\n",
    "# 查看第一行的'TA的年龄_填空1', 'TA的年龄_填空2'\n",
    "print(df_clean10[['TA的年龄_填空1', 'TA的年龄_填空2','TA的身高（单位cm）:底线-最低', 'TA的身高（单位cm）:底线-最高']].head(5))\n",
    "columns_to_swap = [\n",
    "        ('TA的年龄_填空1', 'TA的年龄_填空2'),  # 年龄底线\n",
    "        ('TA的年龄_填空3', 'TA的年龄_填空4'),  # 年龄加分项\n",
    "        ('TA的身高（单位cm）:底线-最低', 'TA的身高（单位cm）:底线-最高'),  # 身高底线\n",
    "        ('TA的身高（单位cm）:加分项-最低', 'TA的身高（单位cm）:加分项-最高'),  # 身高加分项\n",
    "        ('TA的身体质量指数BMI:底线-最低', 'TA的身体质量指数BMI:底线-最高'),  # BMI底线\n",
    "        ('TA的身体质量指数BMI:加分项-最低', 'TA的身体质量指数BMI:加分项-最高'),  # BMI加分项\n",
    "        ('TA在现实生活中成熟的恋爱次数（仅作为加分项）_填空1', 'TA在现实生活中成熟的恋爱次数（仅作为加分项）_填空2'),  # 恋爱次数\n",
    "    ]\n",
    "def 假设我们角色互换 (df:DataFrame, columns_to_swap: list[tuple[str, str]]) ->DataFrame:\n",
    "    # 对一对列进行交换处理\n",
    "    for col1, col2 in columns_to_swap:\n",
    "        if col1 in df.columns and col2 in df.columns:\n",
    "            # 保证 col1 低于 col2\n",
    "            df[[col1, col2]] = df[[col1, col2]].apply(lambda x: pd.Series(sorted(x)), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_clean10 = 假设我们角色互换(df_clean10, columns_to_swap)\n",
    "\n",
    "print(df_clean10[['TA的年龄_填空1', 'TA的年龄_填空2','TA的身高（单位cm）:底线-最低', 'TA的身高（单位cm）:底线-最高']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean11 = df_clean10.copy()\n",
    "# 获取当前列的顺序\n",
    "当前列顺序 = df_clean11.columns.tolist()\n",
    "\n",
    "# 将'id'列移动到第三位置（索引2）\n",
    "当前列顺序.insert(2, 当前列顺序.pop(当前列顺序.index('识别用id')))\n",
    "\n",
    "# 重新排序数据框的列\n",
    "df_clean11 = df_clean11[当前列顺序]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 缩短列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 输出列名和字节长度(df: pd.DataFrame) -> None:\n",
    "    # 创建列名和字节长度的字典\n",
    "    列名和字节长度 = {'列名': [], '字节长度': []}\n",
    "    # 遍历列名\n",
    "    for col in df.columns:\n",
    "        if len(col.encode('utf-8')) >= 50:\n",
    "            # 获取列名的字节长度（UTF-8 编码）\n",
    "            列名和字节长度['列名'].append(col)\n",
    "            列名和字节长度['字节长度'].append(len(col.encode('utf-8')))  # 计算 UTF-8 字节长度\n",
    "            \n",
    "    # 如果字典不是空的，展示并报错\n",
    "    if 列名和字节长度 != {'列名': [], '字节长度': []}:\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "            display(列名和字节长度)\n",
    "            raise ValueError('列名长度超过 50 字节，请修改列名！')\n",
    "    else:\n",
    "        print('所有列名长度均符合要求！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "{'列名': ['底线类生活习惯（给自己打分）:不抽烟就会死',\n",
    "  '底线类生活习惯（给自己打分）:一周不喝酒就会死',\n",
    "  '底线类生活习惯（给自己打分）:一周不吃槟榔就会死',\n",
    "  '底线类生活习惯（给自己打分）:12点前睡觉就会死',\n",
    "  '底线类生活习惯（给自己打分）:10点前不回家会很疲惫',\n",
    "  '底线类生活习惯（给自己打分）:喜欢养宠物',\n",
    "  '底线类生活习惯（设置自己的可接受范围）:不抽烟就会死',\n",
    "  '底线类生活习惯（设置自己的可接受范围）:一周不喝酒就会死',\n",
    "  '底线类生活习惯（设置自己的可接受范围）:一周不吃槟榔就会死',\n",
    "  '底线类生活习惯（设置自己的可接受范围）:12点前睡觉就会死',\n",
    "  '底线类生活习惯（设置自己的可接受范围）:10点前不回家会很疲惫',\n",
    "  '底线类生活习惯（设置自己的可接受范围）:喜欢养宠物']\n",
    "'''\n",
    "\n",
    "# 缩短列名，建立映射关系\n",
    "缩短_列名映射 = {\n",
    "    '经济情况 - 其他信息:是否是独生子女': '经济情况_其他信息:是否是独生子女',\n",
    "    '经济情况 - 其他信息:父母是否都有养老保险': '经济情况_其他信息:父母养老保险',\n",
    "    '经济情况 - 其他信息:是否是单亲家庭': '经济情况_其他信息:是否是单亲家庭',\n",
    "    '生活习惯:掏出去每一分钱都是不得不花的': '生活习惯:掏的钱都是不得不花的',\n",
    "    'TA的地址与你的距离（单位km）:常驻地址相距-底线': 'TA地址与你相距:常驻地址-底线',\n",
    "    'TA的地址与你的距离（单位km）:常驻地址相距-加分项': 'TA地址与你相距:常驻地址-加分项',\n",
    "    'TA的地址与你的距离（单位km）:家乡地址相距-底线': 'TA地址与你相距:家乡地址-底线',\n",
    "    'TA的地址与你的距离（单位km）:家乡地址相距-加分项': 'TA地址与你相距:家乡地址-加分项',\n",
    "    'TA的经济情况 - 其他信息:TA是独生子女': 'TA经济情况_其他信息:TA是独生子女',\n",
    "    'TA的经济情况 - 其他信息:TA的父母有养老保险': 'TA经济情况_其他信息:TA父母养老保险',\n",
    "    'TA的经济情况 - 其他信息:TA的情况是单亲家庭': 'TA经济情况_其他信息:TA是单亲家庭',\n",
    "    'TA在现实生活中成熟的恋爱次数（仅作为加分项）_填空1': 'TA成熟的恋爱次数_填空1',\n",
    "    'TA在现实生活中成熟的恋爱次数（仅作为加分项）_填空2': 'TA成熟的恋爱次数_填空2',\n",
    "    '勾选与TA相符的描述（仅作为加分项）:会做家常菜': '勾选与TA相符的描述:会做家常菜',\n",
    "    '勾选与TA相符的描述（仅作为加分项）:会一种乐器': '勾选与TA相符的描述:会一种乐器',\n",
    "    '勾选与TA相符的描述（仅作为加分项）:唱歌在调上': '勾选与TA相符的描述:唱歌在调上',\n",
    "    '勾选与TA相符的描述（仅作为加分项）:每月会有8h以上时间用于运动': '勾选与TA相符的描述:每月8h以上运动',\n",
    "    '勾选与TA相符的描述（仅作为加分项）:每月会有4h以上时间用于阅读': '勾选与TA相符的描述:每月4h以上阅读',\n",
    "    '勾选与TA相符的描述（仅作为加分项）:大眼睛': '勾选与TA相符的描述:大眼睛',\n",
    "    '勾选与TA相符的描述（仅作为加分项）:爱笑': '勾选与TA相符的描述:爱笑',\n",
    "    '勾选与TA相符的描述（仅作为加分项）:皮肤白': '勾选与TA相符的描述:皮肤白',\n",
    "    '勾选与TA相符的描述（仅作为加分项）:花时间打扮自己': '勾选与TA相符的描述:花时间打扮自己',\n",
    "    '勾选与TA相符的描述（仅作为加分项）:沉默寡言': '勾选与TA相符的描述:沉默寡言',\n",
    "    '勾选与TA相符的描述（仅作为加分项）:乐于分享': '勾选与TA相符的描述:乐于分享',\n",
    "    '勾选与TA相符的描述（仅作为加分项）:抽象乐子人': '勾选与TA相符的描述:抽象乐子人',\n",
    "    '勾选与TA相符的描述（仅作为加分项）:性格内向': '勾选与TA相符的描述:性格内向',\n",
    "    '底线类生活习惯（给自己打分）:不抽烟就会死': '底线类生活习惯:抽烟',\n",
    "    '底线类生活习惯（给自己打分）:一周不喝酒就会死': '底线类生活习惯:喝酒',\n",
    "    '底线类生活习惯（给自己打分）:一周不吃槟榔就会死': '底线类生活习惯:槟榔',\n",
    "    '底线类生活习惯（给自己打分）:12点前睡觉就会死': '底线类生活习惯:熬夜',\n",
    "    '底线类生活习惯（给自己打分）:10点前不回家会很疲惫': '底线类生活习惯:早回家',\n",
    "    '底线类生活习惯（给自己打分）:喜欢养宠物': '底线类生活习惯:养宠物',\n",
    "    '底线类生活习惯（设置自己的可接受范围）:不抽烟就会死': '底线类生活习惯范围:抽烟',\n",
    "    '底线类生活习惯（设置自己的可接受范围）:一周不喝酒就会死': '底线类生活习惯范围:喝酒',\n",
    "    '底线类生活习惯（设置自己的可接受范围）:一周不吃槟榔就会死': '底线类生活习惯范围:槟榔',\n",
    "    '底线类生活习惯（设置自己的可接受范围）:12点前睡觉就会死': '底线类生活习惯范围:熬夜',\n",
    "    '底线类生活习惯（设置自己的可接受范围）:10点前不回家会很疲惫': '底线类生活习惯范围:早回家',\n",
    "    '底线类生活习惯（设置自己的可接受范围）:喜欢养宠物': '底线类生活习惯范围:养宠物'\n",
    "}\n",
    "\n",
    "# 使用 pandas 重命名列名\n",
    "df_clean12 = df_clean11.rename(columns=缩短_列名映射).copy()\n",
    "\n",
    "# 示例使用\n",
    "输出列名和字节长度(df_clean12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_clean12.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned.info())\n",
    "display(df_cleaned.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 问卷无效原因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个空字典，应有两列，第一列是原因，第二列是值，用于存储每一次筛除的结果\n",
    "筛除结果 = {'原因': [], '值': [], '占比': []}\n",
    "print(df.shape[0])\n",
    "\"\"\"\n",
    "原因0:年龄小于18岁，值=df_clean.shape[0] - df_clean0.shape[0]，\n",
    "原因1：答题时长少于2分钟，值=df_clean.shape[0] - df_clean1.shape[0]，\n",
    "占比=（df_clean.shape[0] - df_clean1.shape[0]）/（df.shape[0] - df_cleaned.shape[0]）\n",
    "原因2：IP&ID重复，值=df_clean1.shape[0] - df_clean2.shape[0]，\n",
    "占比=（df_clean1.shape[0] - df_clean2.shape[0]）/（df.shape[0] - df_cleaned.shape[0]）\n",
    "原因3：BMI底线不符合，值=df_clean2.shape[0] - df_clean3.shape[0]，\n",
    "占比=（df_clean2.shape[0] - df_clean3.shape[0]）/（df.shape[0] - df_cleaned.shape[0]）\n",
    "原因4：个人总资产大于家庭总资产，值=df_clean3.shape[0] - df_clean4.shape[0]，\n",
    "占比=（df_clean3.shape[0] - df_clean4.shape[0]）/（df.shape[0] - df_cleaned.shape[0]）\n",
    "原因5：距离底线不符合，值=df_clean4.shape[0] - df_clean5.shape[0]，\n",
    "占比=（df_clean4.shape[0] - df_clean5.shape[0]）/（df.shape[0] - df_cleaned.shape[0]）\n",
    "原因6：ID校验不通过，值=df_clean9.shape[0] - df_clean7.shape[0]，\n",
    "占比=（df_clean8.shape[0] - df_clean6.shape[0]）/（df.shape[0] - df_cleaned.shape[0]）\n",
    "\"\"\"\n",
    "# 输入字典\n",
    "筛除结果['原因'] = ['年龄小于18岁','答题时长少于2分钟', 'IP&ID重复', 'BMI底线不符合', '个人总资产大于家庭总资产', '距离底线不符合', '值超出范围', 'ID校验不通过']\n",
    "筛除结果['值'] = [df_clean.shape[0] - df_clean0.shape[0], \n",
    "             df_clean0.shape[0] - df_clean1.shape[0], \n",
    "             df_clean1.shape[0] - df_clean2.shape[0], \n",
    "             df_clean2.shape[0] - df_clean3.shape[0], \n",
    "             df_clean3.shape[0] - df_clean4.shape[0], \n",
    "             df_clean4.shape[0] - df_clean5.shape[0], \n",
    "             df_clean5.shape[0] - df_clean6.shape[0], \n",
    "             df_clean7.shape[0] - df_clean9.shape[0]]\n",
    "\n",
    "筛除结果['占比'] = [(df_clean.shape[0] - df_clean0.shape[0])/(df.shape[0] - df_cleaned.shape[0]), \n",
    "              (df_clean0.shape[0] - df_clean1.shape[0])/(df.shape[0] - df_cleaned.shape[0]), \n",
    "              (df_clean1.shape[0] - df_clean2.shape[0])/(df.shape[0] - df_cleaned.shape[0]), \n",
    "              (df_clean2.shape[0] - df_clean3.shape[0])/(df.shape[0] - df_cleaned.shape[0]), \n",
    "              (df_clean3.shape[0] - df_clean4.shape[0])/(df.shape[0] - df_cleaned.shape[0]), \n",
    "              (df_clean4.shape[0] - df_clean5.shape[0])/(df.shape[0] - df_cleaned.shape[0]), \n",
    "              (df_clean5.shape[0] - df_clean6.shape[0])/(df.shape[0] - df_cleaned.shape[0]), \n",
    "              (df_clean7.shape[0] - df_clean9.shape[0])/(df.shape[0] - df_cleaned.shape[0])]\n",
    "\n",
    "display(pd.DataFrame(筛除结果))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_导入数据库 = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库连接信息\n",
    "user = 'postgres'\n",
    "password = 'root2'\n",
    "host = 'localhost'\n",
    "port = 5655 # 替换为你的数据库端口号\n",
    "database = 'postgres'  # 替换为你的数据库名称\n",
    "schema = '赛博相亲'  # 替换为你的架构名称\n",
    "table_name = 'v1_2问卷填写数据存储_20250405'  # 替换为你的表名称\n",
    "\n",
    "# 创建 PostgreSQL 数据库连接和引擎\n",
    "try:\n",
    "    # 使用 SQLAlchemy 创建引擎\n",
    "    engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}?options=-csearch_path={schema}')\n",
    "    print(\"SQLAlchemy 引擎创建成功\")\n",
    "\n",
    "    # 检查表是否存在\n",
    "    with engine.begin() as conn:\n",
    "        check_table_query = f\"\"\"\n",
    "        SELECT EXISTS (\n",
    "            SELECT FROM information_schema.tables \n",
    "            WHERE table_schema = '{schema}' \n",
    "            AND table_name = '{table_name}'\n",
    "        );\n",
    "        \"\"\"\n",
    "        table_exists = conn.execute(text(check_table_query)).scalar()\n",
    "\n",
    "        # 根据表的存在性插入数据\n",
    "        if not table_exists:\n",
    "            # 如果表不存在，直接创建表并插入数据\n",
    "            print(f\"表 {schema}.{table_name} 不存在，正在创建...\")\n",
    "            df_导入数据库.to_sql(table_name, conn, schema=schema, if_exists='replace', index=False)\n",
    "            print(f\"表 {schema}.{table_name} 创建并成功插入数据\")\n",
    "        else:\n",
    "            # 如果表存在，避免插入完全重复的数据\n",
    "            print(f\"表 {schema}.{table_name} 已存在，正在检查重复记录...\")\n",
    "            # 将目前的数据替换至数据库中的临时表\n",
    "            temp_table_name = f\"{table_name}_temp\"\n",
    "            \n",
    "            # 获取数据库表的列名\n",
    "            columns_query = f\"\"\"\n",
    "            SELECT column_name \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_schema = '{schema}' AND table_name = '{table_name}'\n",
    "            ORDER BY ordinal_position;\n",
    "            \"\"\"\n",
    "            columns = pd.read_sql(columns_query, conn)['column_name'].tolist()\n",
    "            # 给所有列名加上双引号，避免 SQL 解析错误\n",
    "            columns_str = ', '.join([f'\"{col}\"' for col in columns])\n",
    "            \n",
    "            \n",
    "            df_导入数据库.to_sql(temp_table_name, conn, schema=schema, if_exists='replace', index=False)\n",
    "            # 直接使用sql进行操作，查询临时表与数据库表的差集\n",
    "            diff_query = f\"\"\"\n",
    "            SELECT {columns_str} FROM {schema}.{temp_table_name}\n",
    "            EXCEPT\n",
    "            SELECT {columns_str} FROM {schema}.{table_name};\n",
    "            \"\"\"\n",
    "            diff_df = pd.read_sql(diff_query, conn)\n",
    "            # 如果有差集，插入差集数据\n",
    "            diff_df.to_sql(table_name, conn, schema=schema, if_exists='append', index=False)\n",
    "            # 打印原本有多少行，导入了多少行\n",
    "            print(f\"表 {schema}.{table_name} 原有 {df_导入数据库.shape[0]} 行数据，导入了 {diff_df.shape[0]} 行数据\")\n",
    "            # 如果临时表存在，删除临时表\n",
    "            drop_temp_table_query = f\"\"\"\n",
    "            DROP TABLE IF EXISTS {schema}.{temp_table_name};\n",
    "            \"\"\"\n",
    "            # 删除临时表\n",
    "            conn.execute(text(drop_temp_table_query))\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出库中的数据\n",
    "df_cleaned = pd.read_sql(f\"SELECT * FROM {schema}.{table_name}\", engine)\n",
    "# 去掉括号并拆分为经纬度\n",
    "df_cleaned['常驻坐标'] = df_cleaned['常驻坐标'].str.strip('()')\n",
    "df_cleaned['家乡坐标'] = df_cleaned['家乡坐标'].str.strip('()')\n",
    "\n",
    "# 分离为经度和纬度\n",
    "df_cleaned[['常驻纬度', '常驻经度']] = df_cleaned['常驻坐标'].str.split(',', expand=True).astype(float)\n",
    "df_cleaned[['家乡纬度', '家乡经度']] = df_cleaned['家乡坐标'].str.split(',', expand=True).astype(float)\n",
    "\n",
    "# 生成字段(纬度,经度)，用于计算距离\n",
    "df_cleaned['常驻坐标'] = list(zip(df_cleaned['常驻纬度'], df_cleaned['常驻经度']))\n",
    "df_cleaned['家乡坐标'] = list(zip(df_cleaned['家乡纬度'], df_cleaned['家乡经度']))\n",
    "\n",
    "# 删除经度和纬度列\n",
    "df_cleaned = df_cleaned.drop(columns=['常驻经度', '常驻纬度', '家乡经度', '家乡纬度'])\n",
    "print(df_cleaned.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 仅保留最新数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IP 重复的数据，仅保留结束答题时间最新的一条\n",
    "df_cleaned = df_cleaned.sort_values(by=\"结束答题时间\")\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=[\"IP\"], keep=\"last\")\n",
    "# 平台&识别用id 同时重复的数据，仅保留结束答题时间最新的一条\n",
    "df_cleaned = df_cleaned.sort_values(by=\"结束答题时间\")\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=[\"填写平台\", \"识别用id\"], keep=\"last\")\n",
    "print(f'2：{df_cleaned.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除结束答题时间列和ip列\n",
    "df_cleaned = df_cleaned.drop(columns=['结束答题时间', 'IP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出无效名单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_more_end 中包含的识别用id与填写平台，但不在 df_cleaned 中。\n",
    "## 创建临时用的唯一识别符\n",
    "df_cleaned['唯一识别符'] = df_cleaned['识别用id'] + df_cleaned['填写平台']\n",
    "df_more_end['唯一识别符'] = df_more_end['识别用id'] + df_more_end['填写平台']\n",
    "## 找出 df_more_end 中包含的识别用id与填写平台，但不在 df_cleaned 中。\n",
    "df_无效名单 = df_more_end[~df_more_end['唯一识别符'].isin(df_cleaned['唯一识别符'])]\n",
    "## 删除临时列\n",
    "df_cleaned = df_cleaned.drop(columns=['唯一识别符'])\n",
    "df_more_end = df_more_end.drop(columns=['唯一识别符'])\n",
    "## 创建新列，列内数值为当日日期\n",
    "df_无效名单['日期'] = pd.to_datetime('today').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 pandas 重命名列名\n",
    "df_无效名单 = df_无效名单.rename(columns=缩短_列名映射).copy()\n",
    "\n",
    "# 示例使用\n",
    "输出列名和字节长度(df_无效名单)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无效名单在输出成功数据处输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两两匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_未两两匹配 = df_cleaned.copy()\n",
    "df_未两两匹配 = df_未两两匹配.drop(columns=['答题时长'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成笛卡尔积（两两匹配）\n",
    "df_前置底线筛选 = df_未两两匹配.merge(df_未两两匹配, how='cross')\n",
    "\n",
    "# 保留不相等的情况，分别比较两个字段\n",
    "df_前置底线筛选 = df_前置底线筛选[\n",
    "    (df_前置底线筛选['填写平台_x'] != df_前置底线筛选['填写平台_y']) | \n",
    "    (df_前置底线筛选['识别用id_x'] != df_前置底线筛选['识别用id_y'])\n",
    "]\n",
    "\n",
    "# 输出结果\n",
    "print(df_前置底线筛选.info())\n",
    "print(df_前置底线筛选.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 去除匹配过期的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库连接信息\n",
    "user = 'postgres'\n",
    "password = 'root2'\n",
    "host = 'localhost'\n",
    "port = 5655 # 替换为你的数据库端口\n",
    "database = 'postgres'  # 替换为你的数据库名称\n",
    "schema = '赛博相亲'  # 替换为你的架构名称\n",
    "table_name_过期 = '匹配过期记录'  # 替换为你的表名称\n",
    "\n",
    "# 创建 PostgreSQL 数据库连接\n",
    "try:\n",
    "    conn = psycopg2.connect(database=database, user=user, password=password, host=host, port=port)\n",
    "    conn.autocommit = True  # 如果需要创建数据库，保持自动提交\n",
    "\n",
    "    # 设置搜索路径\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(f'SET search_path TO {schema}')\n",
    "    \n",
    "    print(\"连接成功并已设置搜索路径\")\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"数据库连接失败: {e}\")\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n",
    "\n",
    "# 创建 postgresql 数据库连接\n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}?options=-csearch_path={schema}')\n",
    "\n",
    "# 检查表是否存在\n",
    "with engine.begin() as conn:\n",
    "    匹配过期取数 = f\"\"\"\n",
    "    select * from {schema}.{table_name_过期} where 日期 != current_date\n",
    "    \"\"\"\n",
    "\n",
    "    df_去除匹配过期 = pd.read_sql(匹配过期取数, conn)\n",
    "    \n",
    "    print(f\"表 {schema}.{table_name_过期} 已存在，已取出过期记录\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在df_前置底线筛选中，删除匹配过期的数据\n",
    "df_去除匹配过期 = df_前置底线筛选.merge(df_去除匹配过期, \n",
    "                               left_on=['识别用id_x', '识别用id_y'], \n",
    "                               right_on=['id_x', 'id_y'], \n",
    "                               how='inner')\n",
    "\n",
    "# 删除匹配过期的数据\n",
    "df_前置底线筛选 = df_前置底线筛选.merge(df_去除匹配过期[['识别用id_x', '识别用id_y']], \n",
    "                                   on=['识别用id_x', '识别用id_y'], \n",
    "                                   how='left', \n",
    "                                   indicator=True).query('_merge == \"left_only\"').drop(columns=['_merge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前置纯底线筛选"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 性取向（纯底线）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性别匹配规则（将其改为集合更易于快速查找）\n",
    "性别匹配 = {\n",
    "    (\"A.男\", \"A.男\"),\n",
    "    (\"B.其他\", \"A.男\"),\n",
    "    (\"B.其他\", \"B.其他\"),\n",
    "    (\"B.其他\", \"C.女\"),\n",
    "    (\"C.女\", \"C.女\"),\n",
    "}\n",
    "\n",
    "# 生成性别匹配结果\n",
    "df_前置底线筛选['底线：性取向'] = df_前置底线筛选.apply(\n",
    "    lambda row: (\n",
    "        # 检查生理性别匹配\n",
    "        (row['性别&性取向:TA的生理性别_x'], row['性别&性取向:自身生理性别_y']) in 性别匹配\n",
    "        # 检查心理性别匹配\n",
    "        and (row['性别&性取向:TA的心理性别_x'], row['性别&性取向:自身心理性别_y']) in 性别匹配\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(df_前置底线筛选['底线：性取向'].head(2))\n",
    "# 转换布尔值为0和1\n",
    "df_前置底线筛选['底线：性取向'] = df_前置底线筛选['底线：性取向'].apply(lambda x: 0 if x else 1)\n",
    "print(df_前置底线筛选['底线：性取向'].head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_前置筛选_性取向通过 = df_前置底线筛选[df_前置底线筛选['底线：性取向'] == 0].copy()\n",
    "print(df_前置筛选_性取向通过.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 是否打算在两年内结婚（纯底线）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "结婚意愿匹配 = {\n",
    "    (\"A.两年内结婚\", \"A.两年内结婚\"),\n",
    "    (\"A.两年内结婚\", \"B.看感觉，有合适的人2年内结婚\"),\n",
    "    (\"B.看感觉，有合适的人2年内结婚\", \"A.两年内结婚\"),\n",
    "    (\"B.看感觉，有合适的人2年内结婚\", \"B.看感觉，有合适的人2年内结婚\"),\n",
    "    (\"C.2年内不结婚\", \"C.2年内不结婚\"),\n",
    "}\n",
    "# 生成匹配结果\n",
    "df_前置筛选_性取向通过['底线：是否打算在两年内结婚'] = df_前置筛选_性取向通过.apply(\n",
    "    lambda row: (row['是否打算在2年内结婚？_x'], row['是否打算在2年内结婚？_y']) in 结婚意愿匹配,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(df_前置筛选_性取向通过[['底线：是否打算在两年内结婚','是否打算在2年内结婚？_x','是否打算在2年内结婚？_y']].head(2))\n",
    "# 转换布尔值为0和1\n",
    "df_前置筛选_性取向通过['底线：是否打算在两年内结婚'] = df_前置筛选_性取向通过['底线：是否打算在两年内结婚'].apply(lambda x: 0 if x else 1)\n",
    "print(df_前置筛选_性取向通过['底线：是否打算在两年内结婚'].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_前置筛选_结婚意愿通过 = df_前置筛选_性取向通过[df_前置筛选_性取向通过['底线：是否打算在两年内结婚'] == 0].copy()\n",
    "print(df_前置筛选_结婚意愿通过.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 宗教信仰（纯底线）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"列名\n",
    "宗教信仰_填空1\n",
    "宗教信仰_填空2\n",
    "\"\"\"\n",
    "\n",
    "# 生成匹配结果\n",
    "df_前置筛选_结婚意愿通过['底线：宗教信仰'] = df_前置筛选_结婚意愿通过.apply(\n",
    "    lambda row: True if row['宗教信仰_填空2_x'] == \"是\" else row['宗教信仰_填空1_x'] == row['宗教信仰_填空1_y'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_前置筛选_结婚意愿通过['底线：宗教信仰'] = df_前置筛选_结婚意愿通过['底线：宗教信仰'].apply(lambda x: 0 if x else 1)\n",
    "\n",
    "# 看一看宗教信仰_填空2_x 为否的情况\n",
    "print(df_前置筛选_结婚意愿通过[df_前置筛选_结婚意愿通过['宗教信仰_填空2_x'] == \"否\"][['宗教信仰_填空2_x','宗教信仰_填空1_x','宗教信仰_填空1_y','底线：宗教信仰']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_前置筛选_宗教信仰通过 = df_前置筛选_结婚意愿通过[df_前置筛选_结婚意愿通过['底线：宗教信仰'] == 0].copy()\n",
    "print(df_前置筛选_宗教信仰通过.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 婚姻情况（纯底线）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"列名\n",
    "婚姻情况_填空1\n",
    "婚姻情况_填空2\n",
    "\"\"\"\n",
    "\n",
    "# 生成匹配结果\n",
    "df_前置筛选_宗教信仰通过['底线：婚姻情况'] = df_前置筛选_宗教信仰通过.apply(\n",
    "    lambda row: True if row['婚姻情况_填空2_x'] == \"是\" else row['婚姻情况_填空1_y'] == \"未婚\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_前置筛选_宗教信仰通过['底线：婚姻情况'] = df_前置筛选_宗教信仰通过['底线：婚姻情况'].apply(lambda x: 0 if x else 1)\n",
    "\n",
    "# 看一看\n",
    "print(df_前置筛选_宗教信仰通过[df_前置筛选_宗教信仰通过['婚姻情况_填空2_x'] == \"否\"][['婚姻情况_填空2_x','婚姻情况_填空1_x','婚姻情况_填空1_y','底线：婚姻情况']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_前置筛选_婚姻情况通过 = df_前置筛选_宗教信仰通过[df_前置筛选_宗教信仰通过['底线：婚姻情况'] == 0].copy()\n",
    "print(df_前置筛选_婚姻情况通过.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_前置筛选_结果 = df_前置筛选_婚姻情况通过.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_前置筛选_结果.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成交互列（额外信息）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cartesian_more = df_前置筛选_结果.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据经纬度计算距离，计算结果单位为公里，保留两位小数\n",
    "df_cartesian_more['常驻距离'] = df_前置筛选_结果.apply(\n",
    "    lambda row: round(\n",
    "        haversine(row['常驻坐标_x'], row['常驻坐标_y'], unit=Unit.KILOMETERS), 2\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "df_cartesian_more['家乡距离'] = df_前置筛选_结果.apply(\n",
    "    lambda row: round(\n",
    "        haversine(row['家乡坐标_x'], row['家乡坐标_y'], unit=Unit.KILOMETERS), 2\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示结果\n",
    "print(df_cartesian_more[['常驻坐标_x','常驻坐标_y','家乡坐标_x','家乡坐标_y','常驻距离', '家乡距离']].head())\n",
    "# 已验证，结果误差小于1km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看现有列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示完整列名\n",
    "with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "    display(df_cartesian_more.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算匹配结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本原则：只判断自己是否匹配对方，不判断对方是否匹配自己。\n",
    "因为在对方的那一行，ta也会进行匹配，如果不匹配会自动筛除，没必要重复运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_底线 = df_cartesian_more\n",
    "print(df_底线.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 年龄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成年龄匹配结果\n",
    "df_底线['底线：年龄'] = df_底线.apply(\n",
    "    lambda row: row['TA的年龄_填空1_x'] <= row['出生年份_y'] <= row['TA的年龄_填空2_x'] ,\n",
    "    axis=1\n",
    ") # 如果ta的年龄在我的要求内，返回True，否则返回False\n",
    "# 加分项\n",
    "df_底线['加分项：年龄'] = df_底线.apply(\n",
    "    lambda row: row['TA的年龄_填空3_x'] <= row['出生年份_y'] <= row['TA的年龄_填空4_x'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_底线['底线：年龄'] = df_底线['底线：年龄'].apply(lambda x: 0 if x else 1)\n",
    "df_底线['加分项：年龄'] = df_底线['加分项：年龄'].apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 身高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成身高匹配结果\n",
    "df_底线['底线：身高'] = df_底线.apply(\n",
    "    lambda row: row['TA的身高（单位cm）:底线-最低_x'] <= row['身高（单位cm）_y'] <= row['TA的身高（单位cm）:底线-最高_x'] ,\n",
    "    axis=1\n",
    ")\n",
    "# 加分项\n",
    "df_底线['加分项：身高'] = df_底线.apply(\n",
    "    lambda row: row['TA的身高（单位cm）:加分项-最低_x'] <= row['身高（单位cm）_y'] <= row['TA的身高（单位cm）:加分项-最高_x'] ,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_底线['底线：身高'] = df_底线['底线：身高'].apply(lambda x: 0 if x else 1)\n",
    "df_底线['加分项：身高'] = df_底线['加分项：身高'].apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成BMI匹配结果\n",
    "df_底线['底线：BMI'] = df_底线.apply(\n",
    "    lambda row: row['TA的身体质量指数BMI:底线-最低_x'] <= row['BMI_y'] <= row['TA的身体质量指数BMI:底线-最高_x'],\n",
    "    axis=1\n",
    ")\n",
    "# 加分项\n",
    "df_底线['加分项：BMI'] = df_底线.apply(\n",
    "    lambda row: row['TA的身体质量指数BMI:加分项-最低_x'] <= row['BMI_y'] <= row['TA的身体质量指数BMI:加分项-最高_x'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_底线['底线：BMI'] = df_底线['底线：BMI'].apply(lambda x: 0 if x else 1)\n",
    "df_底线['加分项：BMI'] = df_底线['加分项：BMI'].apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取“学历”中的前缀（例如 'A'）\n",
    "df_底线['学历前缀_y'] = df_底线['学历_y'].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "# 对每一行进行判断，学历是否在TA的学历底线要求之内\n",
    "df_底线['底线：学历'] = df_底线.apply(\n",
    "    lambda row: row['学历前缀_y'] in row['TA的学历（底线）_x'].split('.'),\n",
    "    axis=1\n",
    ")\n",
    "# 加分项\n",
    "df_底线['加分项：学历'] = df_底线.apply(\n",
    "    lambda row: row['学历前缀_y'] in row['TA的学历（加分项）_x'].split('.'),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_底线['底线：学历'] = df_底线['底线：学历'].apply(lambda x: 0 if x else 1)\n",
    "df_底线['加分项：学历'] = df_底线['加分项：学历'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# 删除辅助列\n",
    "df_底线.drop(columns=['学历前缀_y'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 工作性质（纯加分项）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取“工作性质”中的前缀（例如 'A'）\n",
    "df_底线['工作性质前缀_y'] = df_底线['工作性质_y'].apply(lambda x: x.split('.')[0])\n",
    "print(df_底线[['工作性质前缀_y','工作性质_y']].head(2))\n",
    "\n",
    "# 对每一行进行判断，工作性质是否在TA的工作性质加分项要求之内\n",
    "df_底线['加分项：工作性质'] = df_底线.apply(\n",
    "    lambda row: row['工作性质前缀_y'] in row['TA的工作性质（仅作为加分项）_x'].split('.'),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_底线['加分项：工作性质'] = df_底线['加分项：工作性质'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# 删除辅助列\n",
    "df_底线.drop(columns=['工作性质前缀_y'], inplace=True)\n",
    "print(df_底线[['加分项：工作性质','工作性质_y','TA的工作性质（仅作为加分项）_x']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_底线['底线：常驻距离'] = df_底线.apply(\n",
    "    lambda row: (\n",
    "        (row['常驻距离'] <= row['TA地址与你相距:常驻地址-底线_x'] and \n",
    "        row['常驻市_x'] == row['常驻市_y'])\n",
    "        if row['TA地址与你相距:常驻地址-底线_x'] <= 60 \n",
    "        else row['常驻距离'] <= row['TA地址与你相距:常驻地址-底线_x']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_底线['底线：常驻距离'] = df_底线['底线：常驻距离'].apply(lambda x: 0 if x else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_底线['底线：家乡距离'] = df_底线.apply(\n",
    "    lambda row: (\n",
    "        (row['家乡距离'] <= row['TA地址与你相距:家乡地址-底线_x'] and \n",
    "        row['家乡市_x'] == row['家乡市_y'])\n",
    "        if row['TA地址与你相距:家乡地址-底线_x'] <= 60 \n",
    "        else row['家乡距离'] <= row['TA地址与你相距:家乡地址-底线_x']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_底线['底线：家乡距离'] = df_底线['底线：家乡距离'].apply(lambda x: 0 if x else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_底线['加分项：常驻距离'] = df_底线.apply(\n",
    "    lambda row: row['常驻距离'] <= row['TA地址与你相距:常驻地址-加分项_x'],\n",
    "    axis=1\n",
    ")\n",
    "df_底线['加分项：家乡距离'] = df_底线.apply(\n",
    "    lambda row: row['家乡距离'] <= row['TA地址与你相距:家乡地址-加分项_x'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "\n",
    "df_底线['加分项：常驻距离'] = df_底线['加分项：常驻距离'].apply(lambda x: 1 if x else 0)\n",
    "df_底线['加分项：家乡距离'] = df_底线['加分项：家乡距离'].apply(lambda x: 1 if x else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 月均收入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成收入匹配结果\n",
    "df_底线['底线：月均收入'] = df_底线.apply(\n",
    "    lambda row: row['月均收入（单位：元）_y'] >= row['TA的月均收入（单位：元）_填空1_x'] ,\n",
    "    axis=1\n",
    ")\n",
    "# 加分项\n",
    "df_底线['加分项：月均收入'] = df_底线.apply(\n",
    "    lambda row: row['月均收入（单位：元）_y'] >= row['TA的月均收入（单位：元）_填空1_x'] ,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_底线['底线：月均收入'] = df_底线['底线：月均收入'].apply(lambda x: 0 if x else 1)\n",
    "df_底线['加分项：月均收入'] = df_底线['加分项：月均收入'].apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 个人&家庭总资产"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_底线['底线：个人总资产'] = df_底线.apply(\n",
    "    lambda row: row['个人总资产（单位：万元）_y'] >= row['TA的个人总资产（单位：万元）_填空1_x'],\n",
    "    axis=1\n",
    ")\n",
    "df_底线['加分项：个人总资产'] = df_底线.apply(\n",
    "    lambda row: row['个人总资产（单位：万元）_y'] >= row['TA的个人总资产（单位：万元）_填空2_x'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 家庭总资产\n",
    "df_底线['底线：家庭总资产'] = df_底线.apply(\n",
    "    lambda row: row['家庭总资产（单位：万元）_y'] >= row['TA的家庭总资产（单位：万元）_填空1_x'],\n",
    "    axis=1\n",
    ")\n",
    "# 加分项\n",
    "df_底线['加分项：家庭总资产'] = df_底线.apply(\n",
    "    lambda row: row['家庭总资产（单位：万元）_y'] >= row['TA的家庭总资产（单位：万元）_填空2_x'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_底线['底线：个人总资产'] = df_底线['底线：个人总资产'].apply(lambda x: 0 if x else 1)\n",
    "df_底线['加分项：个人总资产'] = df_底线['加分项：个人总资产'].apply(lambda x: 1 if x else 0)\n",
    "df_底线['底线：家庭总资产'] = df_底线['底线：家庭总资产'].apply(lambda x: 0 if x else 1)\n",
    "df_底线['加分项：家庭总资产'] = df_底线['加分项：家庭总资产'].apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 经济情况 - 其他信息（纯底线）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"列名\n",
    "经济情况 - 其他信息:是否是独生子女\n",
    "经济情况 - 其他信息:父母是否有养老保险\n",
    "经济情况_其他信息:是否是单亲家庭\n",
    "TA的经济情况 - 其他信息:TA是独生子女\n",
    "TA的经济情况 - 其他信息:TA的父母有养老保险\n",
    "TA经济情况_其他信息:TA是单亲家庭\n",
    "\"\"\"\n",
    "# 定义匹配规则（将其改为集合更易于快速查找）\n",
    "其他经济情况匹配 = {\n",
    "    (\"A.是\", \"A.是\"),\n",
    "    (\"B.两者皆可\", \"A.是\"),\n",
    "    (\"B.两者皆可\", \"B.否\"),\n",
    "    (\"C.否\", \"B.否\"),\n",
    "}\n",
    "# 生成匹配结果\n",
    "df_底线['底线：是否是独生子女'] = df_底线.apply(\n",
    "    lambda row: (row['TA经济情况_其他信息:TA是独生子女_x'], row['经济情况_其他信息:是否是独生子女_y']) in 其他经济情况匹配,\n",
    "    axis=1\n",
    ")\n",
    "df_底线['底线：父母是否有养老保险'] = df_底线.apply(\n",
    "    lambda row: (row['TA经济情况_其他信息:TA父母养老保险_x'], row['经济情况_其他信息:父母养老保险_y']) in 其他经济情况匹配,\n",
    "    axis=1\n",
    ")\n",
    "df_底线['底线：是否是单亲家庭'] = df_底线.apply(\n",
    "    lambda row: (row['TA经济情况_其他信息:TA是单亲家庭_x'], row['经济情况_其他信息:是否是单亲家庭_y']) in 其他经济情况匹配,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(df_底线['底线：是否是独生子女'].head(2))\n",
    "print(df_底线['底线：父母是否有养老保险'].head(2))\n",
    "print(df_底线['底线：是否是单亲家庭'].head(2))\n",
    "# 转换布尔值为0和1\n",
    "df_底线['底线：是否是独生子女'] = df_底线['底线：是否是独生子女'].apply(lambda x: 0 if x else 1)\n",
    "df_底线['底线：父母是否有养老保险'] = df_底线['底线：父母是否有养老保险'].apply(lambda x: 0 if x else 1)\n",
    "df_底线['底线：是否是单亲家庭'] = df_底线['底线：是否是单亲家庭'].apply(lambda x: 0 if x else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 工作强度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成工作强度匹配结果\n",
    "df_底线['底线：工作强度'] = df_底线.apply(\n",
    "    lambda row: (row['工作强度_填空1_y'] <= row['TA的工作强度_填空1_x']) and\n",
    "                (row['工作强度_填空2_y'] <= row['TA的工作强度_填空2_x']) ,\n",
    "    axis=1\n",
    ")\n",
    "# 加分项\n",
    "df_底线['加分项：工作强度'] = df_底线.apply(\n",
    "    lambda row: (row['工作强度_填空1_y'] <= row['TA的工作强度_填空3_x']) and\n",
    "                (row['工作强度_填空2_y'] <= row['TA的工作强度_填空4_x']) ,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_底线['底线：工作强度'] = df_底线['底线：工作强度'].apply(lambda x: 0 if x else 1)\n",
    "df_底线['加分项：工作强度'] = df_底线['加分项：工作强度'].apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 婚姻观&生育观（纯底线）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"列名\n",
    "婚姻观&生育观:丁克\n",
    "婚姻观&生育观:支持婚前性生活\n",
    "婚姻观&生育观:婚后和父母一起住\n",
    "\"\"\"\n",
    "# 定义匹配规则（将其改为集合更易于快速查找）\n",
    "# 此处是神奇的双方同时进行筛选，而非单方面筛选，神奇吧！\n",
    "婚姻观生育观不匹配 = {\n",
    "    (\"A.我希望双方都\", \"C.我自己不想\"),\n",
    "    (\"C.我自己不想\", \"A.我希望双方都\")\n",
    "}\n",
    "# 生成匹配结果\n",
    "df_底线['底线：婚姻观&生育观'] = df_底线.apply(\n",
    "    lambda row: ((row['婚姻观&生育观:丁克_x'], row['婚姻观&生育观:丁克_y']) in 婚姻观生育观不匹配) or\n",
    "                ((row['婚姻观&生育观:支持婚前性生活_x'], row['婚姻观&生育观:支持婚前性生活_y']) in 婚姻观生育观不匹配) or\n",
    "                ((row['婚姻观&生育观:婚后和父母一起住_x'], row['婚姻观&生育观:婚后和父母一起住_y']) in 婚姻观生育观不匹配),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1，因为这一次是in不匹配，所以取反\n",
    "df_底线['底线：婚姻观&生育观'] = df_底线['底线：婚姻观&生育观'].apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 家务参与意愿（纯加分项）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"列名\n",
    "家务参与意愿:参与日常烧饭\n",
    "家务参与意愿:参与日常清洁\n",
    "家务参与意愿:参与洗衣洗鞋\n",
    "家务参与意愿:参与家具维修\n",
    "\"\"\"\n",
    "\n",
    "# 生成匹配结果\n",
    "df_底线['加分项：家务参与意愿'] = df_底线.apply(\n",
    "lambda row: (\n",
    "                int((row['家务参与意愿:参与日常烧饭_x'] + row['家务参与意愿:参与日常烧饭_y']) >= 5) +\n",
    "                int((row['家务参与意愿:参与日常清洁_x'] + row['家务参与意愿:参与日常清洁_y']) >= 5) +\n",
    "                int((row['家务参与意愿:参与洗衣洗鞋_x'] + row['家务参与意愿:参与洗衣洗鞋_y']) >= 5) +\n",
    "                int((row['家务参与意愿:参与家具维修_x'] + row['家务参与意愿:参与家具维修_y']) >= 5)\n",
    "            ) / 4, # 标准化，使得全都符合的情况下数值为 1\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "display(df_底线[['家务参与意愿:参与日常烧饭_x','家务参与意愿:参与日常烧饭_y','家务参与意愿:参与日常清洁_x','家务参与意愿:参与日常清洁_y','家务参与意愿:参与洗衣洗鞋_x','家务参与意愿:参与洗衣洗鞋_y','家务参与意愿:参与家具维修_x','家务参与意愿:参与家具维修_y','加分项：家务参与意愿']].head(2))\n",
    "\n",
    "# 展示加分项的分布（不添加列，全在display中处理\n",
    "display(df_底线['加分项：家务参与意愿'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 恋爱次数（纯加分项）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_底线['加分项：恋爱次数'] = df_底线.apply(\n",
    "    lambda row:\n",
    "        row['TA成熟的恋爱次数_填空1_x'] <= row['在现实生活中成熟的恋爱次数_y'] <= row['TA成熟的恋爱次数_填空2_x'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_底线['加分项：恋爱次数'] = df_底线['加分项：恋爱次数'].apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 金钱观（纯底线）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"列名\n",
    " '金钱观:为买房贷款',\n",
    " '金钱观:为买车贷款',\n",
    " '金钱观:为非必要消费贷款',\n",
    " '金钱观:为金融投资贷款',\n",
    " '金钱观:为实体投资贷款',\n",
    " '金钱观:为还贷款贷款',\n",
    " '金钱观:投资基金',\n",
    " '金钱观:投资股票',\n",
    " '金钱观:创业',\n",
    "\"\"\"\n",
    "# 定义匹配规则（将其改为集合更易于快速查找）\n",
    "# 此处是神奇的双方同时进行筛选，而非单方面筛选，神奇吧！\n",
    "金钱观不匹配 = {\n",
    "    (\"A.我接受\", \"C.我很讨厌这样的人\"),\n",
    "    (\"C.我很讨厌这样的人\", \"A.我接受\")\n",
    "}\n",
    "# 生成匹配结果\n",
    "df_底线['底线：金钱观'] = df_底线.apply(\n",
    "    lambda row: ((row['金钱观:为买房贷款_x'], row['金钱观:为买房贷款_y']) in 金钱观不匹配) or\n",
    "                ((row['金钱观:为买车贷款_x'], row['金钱观:为买车贷款_y']) in 金钱观不匹配) or\n",
    "                ((row['金钱观:为非必要消费贷款_x'], row['金钱观:为非必要消费贷款_y']) in 金钱观不匹配) or\n",
    "                ((row['金钱观:为金融投资贷款_x'], row['金钱观:为金融投资贷款_y']) in 金钱观不匹配) or\n",
    "                ((row['金钱观:为实体投资贷款_x'], row['金钱观:为实体投资贷款_y']) in 金钱观不匹配) or\n",
    "                ((row['金钱观:为还贷款贷款_x'], row['金钱观:为还贷款贷款_y']) in 金钱观不匹配) or\n",
    "                ((row['金钱观:投资基金_x'], row['金钱观:投资基金_y']) in 金钱观不匹配) or\n",
    "                ((row['金钱观:投资股票_x'], row['金钱观:投资股票_y']) in 金钱观不匹配) or\n",
    "                ((row['金钱观:创业_x'], row['金钱观:创业_y']) in 金钱观不匹配)\n",
    "    ,axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1，因为这一次是in不匹配，所以取反\n",
    "df_底线['底线：金钱观'] = df_底线['底线：金钱观'].apply(lambda x: 1 if x else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 底线：生活习惯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"打印列名的数据类型\n",
    "'底线类生活习惯:抽烟',\n",
    "'底线类生活习惯:喝酒',\n",
    "'底线类生活习惯:槟榔',\n",
    "'底线类生活习惯:熬夜',\n",
    "'底线类生活习惯:早回家',\n",
    "'底线类生活习惯:养宠物',\n",
    "'底线类生活习惯范围:抽烟',\n",
    "'底线类生活习惯范围:喝酒',\n",
    "'底线类生活习惯范围:槟榔',\n",
    "'底线类生活习惯范围:养宠物',\n",
    "\"\"\"\n",
    "print(df_底线[['底线类生活习惯:抽烟_x','底线类生活习惯:喝酒_x','底线类生活习惯:槟榔_x','底线类生活习惯:养宠物_x', '底线类生活习惯范围:抽烟_x','底线类生活习惯范围:喝酒_x','底线类生活习惯范围:槟榔_x', '底线类生活习惯范围:养宠物_x']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"列名\n",
    "'底线类生活习惯:抽烟',\n",
    "'底线类生活习惯:喝酒',\n",
    "'底线类生活习惯:槟榔',\n",
    "'底线类生活习惯:养宠物',\n",
    "'底线类生活习惯范围:抽烟',\n",
    "'底线类生活习惯范围:喝酒',\n",
    "'底线类生活习惯范围:槟榔',\n",
    "'底线类生活习惯范围:养宠物',\n",
    "\"\"\"\n",
    "\n",
    "# 生成匹配结果，匹配规则为两者差值小于范围\n",
    "df_底线['底线：生活习惯'] = df_底线.apply(\n",
    "    lambda row: abs(row['底线类生活习惯:抽烟_x'] - row['底线类生活习惯:抽烟_y']) < row['底线类生活习惯范围:抽烟_x'] and\n",
    "                abs(row['底线类生活习惯:喝酒_x'] - row['底线类生活习惯:喝酒_y']) < row['底线类生活习惯范围:喝酒_x'] and\n",
    "                abs(row['底线类生活习惯:槟榔_x'] - row['底线类生活习惯:槟榔_y']) < row['底线类生活习惯范围:槟榔_x'] and\n",
    "                abs(row['底线类生活习惯:养宠物_x'] - row['底线类生活习惯:养宠物_y']) < row['底线类生活习惯范围:养宠物_x'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_底线['底线：生活习惯'] = df_底线['底线：生活习惯'].apply(lambda x: 0 if x else 1)\n",
    "\n",
    "# 打印数据框的前两行，检查结果\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df_底线[[\n",
    "        '底线：生活习惯',\n",
    "        '底线类生活习惯:抽烟_x','底线类生活习惯:抽烟_y','底线类生活习惯范围:抽烟_x',\n",
    "        '底线类生活习惯:喝酒_x','底线类生活习惯:喝酒_y','底线类生活习惯范围:喝酒_x',\n",
    "        '底线类生活习惯:槟榔_x','底线类生活习惯:槟榔_y','底线类生活习惯范围:槟榔_x',\n",
    "        '底线类生活习惯:养宠物_x','底线类生活习惯:养宠物_y','底线类生活习惯范围:养宠物_x'\n",
    "    ]].head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加减分项目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算分数函数\n",
    "def 加减分项目算分(row: pd.Series, columns: list, 完全相反的判定规则: set) -> int:\n",
    "    score = 0\n",
    "    for col in columns:\n",
    "        if (row[f'{col}_x'], row[f'{col}_y']) in 完全相反的判定规则:  # 完全相反，扣1分\n",
    "            score -= 1\n",
    "        elif row[f'{col}_x'] == row[f'{col}_y']:  # 完全相同，加1分\n",
    "            score += 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 没看懂，让ai写的，爱咋咋吧。应该是对的吧？？？\n",
    "def 标准化映射_负值为0(series):\n",
    "    if series.std() == 0:\n",
    "        return series.apply(lambda x: 0.5 if x >= 0 else 0)\n",
    "\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    z_scores = (series - mean) / std\n",
    "    cdf_values = norm.cdf(z_scores)\n",
    "\n",
    "    # 把原始为负的项标记为 0，其余做归一化\n",
    "    final_scores = np.array([cdf if orig >= 0 else 0 for orig, cdf in zip(series, cdf_values)])\n",
    "\n",
    "    # 除掉原始负值后剩下的最大最小\n",
    "    valid_mask = np.array(series) >= 0\n",
    "    valid_scores = final_scores[valid_mask]\n",
    "\n",
    "    if valid_scores.max() - valid_scores.min() == 0:\n",
    "        return pd.Series([0.5 if orig >= 0 else 0 for orig in series], index=series.index)\n",
    "\n",
    "    # 对非负项做归一化处理，使最大为1，最小为0\n",
    "    final_scores[valid_mask] = (valid_scores - valid_scores.min()) / (valid_scores.max() - valid_scores.min())\n",
    "    \n",
    "    # 保留两位小数\n",
    "    final_scores = np.round(final_scores, 1)\n",
    "\n",
    "    return pd.Series(final_scores, index=series.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 饮食习惯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 饮食习惯匹配规则\n",
    "饮食习惯完全相反 = {\n",
    "    (\"A.三天不吃上房揭瓦\", \"C.沾上一点就想漱口\"),\n",
    "    (\"C.沾上一点就想漱口\", \"A.三天不吃上房揭瓦\")\n",
    "}\n",
    "# 定义列名\n",
    "饮食习惯列名 = [\n",
    "    '饮食习惯:辣',\n",
    "    '饮食习惯:香菜',\n",
    "    '饮食习惯:葱',\n",
    "    '饮食习惯:蒜',\n",
    "    '饮食习惯:姜',\n",
    "    '饮食习惯:榴莲',\n",
    "    '饮食习惯:螺狮粉',\n",
    "    '饮食习惯:海鲜/生鲜',\n",
    "]\n",
    "# 计算饮食习惯分数\n",
    "df_底线['加分项：饮食习惯'] = df_底线.apply(\n",
    "    lambda row: 加减分项目算分(row, 饮食习惯列名, 饮食习惯完全相反),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_底线['加分项：饮食习惯'] = 标准化映射_负值为0(df_底线['加分项：饮食习惯'])\n",
    "\n",
    "# 打印数据框的前两行，检查结果\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df_底线[[\n",
    "        '加分项：饮食习惯',\n",
    "        *[f'{col}_x' for col in 饮食习惯列名],\n",
    "        *[f'{col}_y' for col in 饮食习惯列名]\n",
    "    ]].head(2))\n",
    "    \n",
    "# 计算饮食习惯分数的分布\n",
    "display(df_底线['加分项：饮食习惯'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 生活习惯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 饮食习惯匹配规则\n",
    "生活习惯完全相反 = {\n",
    "    (\"A.这是我本人\", \"C.我很讨厌这样的人\"),\n",
    "    (\"C.我很讨厌这样的人\", \"A.这是我本人\")\n",
    "}\n",
    "# 定义列名\n",
    "生活习惯列名 = [\n",
    "    '生活习惯:非常在乎时间观念',\n",
    "    '生活习惯:掏的钱都是不得不花的',\n",
    "    '生活习惯:24小时离不开对方',\n",
    "    '生活习惯:公交能到的地方绝对不打车',\n",
    "    '生活习惯:足不出户',\n",
    "    '生活习惯:二次元',\n",
    "    '生活习惯:一周扔垃圾袋5次及以上',\n",
    "    '生活习惯:重视所有特殊的日子',\n",
    "]\n",
    "# 计算生活习惯分数\n",
    "df_底线['加分项：生活习惯'] = df_底线.apply(\n",
    "    lambda row: 加减分项目算分(row, 生活习惯列名, 生活习惯完全相反),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_底线['加分项：生活习惯'] = 标准化映射_负值为0(df_底线['加分项：生活习惯'])\n",
    "\n",
    "\n",
    "\n",
    "# 打印数据框的前两行，检查结果\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df_底线[[\n",
    "        '加分项：生活习惯',\n",
    "        *[f'{col}_x' for col in 生活习惯列名],\n",
    "        *[f'{col}_y' for col in 生活习惯列名]\n",
    "    ]].head(3))\n",
    "    \n",
    "display(df_底线['加分项：生活习惯'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MBTI（纯加分项）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 计算MBTI分数(mbti: str, ta_mbti: str) -> int:\n",
    "    return all(mbti_char == ta_mbti_char or ta_mbti_char == 'O' \n",
    "               for mbti_char, ta_mbti_char in zip(mbti, ta_mbti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_底线['加分项：MBTI'] = df_底线.apply(\n",
    "    lambda row: 计算MBTI分数(row['MBTI_y'], row['TA的MBTI_x']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 转换布尔值为0和1\n",
    "df_底线['加分项：MBTI'] = df_底线['加分项：MBTI'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "print(df_底线[['加分项：MBTI','MBTI_y','TA的MBTI_x']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多选匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 多选匹配函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 多选匹配(匹配条件1: str, 匹配条件2: str) -> int:\n",
    "    \"\"\"\n",
    "    多选匹配函数，用于匹配多选题的匹配结果。\n",
    "    \n",
    "    参数:\n",
    "    - 匹配条件1 (str): 第一个匹配条件字符串，形如 \"A.B.C.\"。\n",
    "    - 匹配条件2 (str): 第二个匹配条件字符串，形如 \"B.C.D.\"。\n",
    "    \n",
    "    返回值:\n",
    "    - int: 匹配的项目数。\n",
    "    \"\"\"\n",
    "    # 提取完整的 [A-Z]. 格式的选项\n",
    "    set1 = set(re.findall(r'[A-Z]\\.', 匹配条件1))\n",
    "    set2 = set(re.findall(r'[A-Z]\\.', 匹配条件2))\n",
    "    \n",
    "    # 计算交集的长度\n",
    "    多选匹配结果 = len(set1 & set2)\n",
    "    \n",
    "    return 多选匹配结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 相符描述（纯加分项）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相符描述中若有相同字母，加分项\n",
    "df_底线['加分项：相符描述'] = df_底线.apply(\n",
    "    lambda row: 多选匹配(row['相符描述_y'], row['TA的相符描述_x']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_底线['加分项：相符描述'] = 标准化映射_负值为0(df_底线['加分项：相符描述'])\n",
    "\n",
    "display(df_底线[['加分项：相符描述']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 兴趣爱好（纯加分项）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相符描述中若有相同字母，加分项\n",
    "df_底线['加分项：兴趣爱好'] = df_底线.apply(\n",
    "    lambda row: 多选匹配(row['兴趣爱好_y'], row['兴趣爱好_x']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_底线['加分项：兴趣爱好'] = 标准化映射_负值为0(df_底线['加分项：兴趣爱好'])\n",
    "\n",
    "display(df_底线[['加分项：兴趣爱好']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 打分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_打分 = df_底线.copy()\n",
    "df_打分.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据底线筛除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "放弃一个底线映射 = {\n",
    "    '年龄': '底线：年龄',\n",
    "    '身高': '底线：身高',\n",
    "    'BMI': '底线：BMI',\n",
    "    '学历': '底线：学历',\n",
    "    '月均收入': '底线：月均收入',\n",
    "    '个人资产': '底线：个人总资产',\n",
    "    '家庭资产': '底线：家庭总资产',\n",
    "    '经济情况-其他信息': ['底线：是否是独生子女', '底线：父母是否有养老保险', '底线：是否是单亲家庭'],\n",
    "    '工作强度': '底线：工作强度',\n",
    "    '婚姻观&生育观': '底线：婚姻观&生育观',\n",
    "    '金钱观': '底线：金钱观',\n",
    "    '底线类生活习惯': '底线：生活习惯',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 提取底线字段(放弃项):\n",
    "    if pd.isna(放弃项):\n",
    "        return []\n",
    "    # 匹配格式如 'I.工作强度' 或 'L.底线类生活习惯'\n",
    "    match = re.search(r'[A-Z]\\.(.+)', 放弃项)\n",
    "    if match:\n",
    "        项名 = match.group(1).strip()\n",
    "        映射字段 = 放弃一个底线映射.get(项名, None)\n",
    "        if isinstance(映射字段, list):\n",
    "            return 映射字段\n",
    "        elif 映射字段:\n",
    "            return [映射字段]\n",
    "    return []\n",
    "\n",
    "# 匹配所有开头为\"底线：\"的列\n",
    "底线列 = df_打分.columns[df_打分.columns.str.startswith(\"底线：\")]\n",
    "\n",
    "def 获取重要底线列(row, 底线列):\n",
    "    放弃字段列表 = 提取底线字段(row['放弃一个底线项_x'])\n",
    "    return [col for col in 底线列 if col not in 放弃字段列表]\n",
    "\n",
    "def 计算匹配系数(row):\n",
    "    重要列 = 获取重要底线列(row, 底线列)\n",
    "    底线乘积 = row[重要列].sum()\n",
    "    return 1 if 底线乘积 == 0 else 0\n",
    "\n",
    "df_打分['底线匹配系数'] = df_打分.apply(计算匹配系数, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置基本分数\n",
    "基本分数 = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示所有加分项列\n",
    "加分项列 = df_打分.columns[df_打分.columns.str.startswith(\"加分项：\")]\n",
    "display(df_打分[加分项列].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 家务参与意愿是新进入加分项的，它忘了写进赋权题目里了。就默认算进生活习惯里吧\n",
    "计分映射 = {\n",
    "    '年龄': '加分项：年龄',\n",
    "    '身材': ['加分项：身高', '加分项：BMI'] ,\n",
    "    '距离': ['加分项：常驻距离', '加分项：家乡距离'] ,\n",
    "    '个人能力': ['加分项：学历', '加分项：月均收入', '加分项：工作性质'],\n",
    "    '资产情况': ['加分项：个人总资产', '加分项：家庭总资产'],\n",
    "    '工作强度': '加分项：工作强度',\n",
    "    '生活习惯': ['加分项：饮食习惯', '加分项：生活习惯', '加分项：家务参与意愿'],\n",
    "    '性格爱好': ['加分项：MBTI', '加分项：相符描述', '加分项：兴趣爱好'],\n",
    "    '恋爱次数': '加分项：恋爱次数',\n",
    "}\n",
    "\n",
    "'''\n",
    "赋权排序_json 字段中有关于加分项的排序，1 最高，2 次之，9 最低。\n",
    "为方便计算，应将其倒过来，把 1 变成 9，2 变成 8，依此类推，然后再除以整个列别内值的数量 以生成权重\n",
    "再将权重乘以分数，把加分项的分数加起来。\n",
    "将其分布在 0-65 之间，最后加上 底线匹配系数 * 基本分数。\n",
    "'''\n",
    "\n",
    "赋权项总数 = len(计分映射)  # 9\n",
    "加分上限 = 100 - 基本分数  # 加分段总分上限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 计算加分原始得分(row):\n",
    "    # 1. 解析排序字典\n",
    "    try:\n",
    "        排序字典 = json.loads(row['赋权排序_json_x'])\n",
    "    except:\n",
    "        display(row['赋权排序_json_x'])\n",
    "        raise ValueError(f\"json 解析错误\")\n",
    "\n",
    "    分数 = 0\n",
    "\n",
    "    for 项, 排序值 in 排序字典.items():\n",
    "        # 2. 倒序权重：1 → 9，9 → 1\n",
    "        倒序权重 = 赋权项总数 - 排序值 + 1\n",
    "        权重 = 倒序权重 / 赋权项总数  # 归一化到 0~1\n",
    "\n",
    "        对应列 = 计分映射.get(项)\n",
    "        if 对应列 is None:\n",
    "            raise ValueError(f\"未知的加分项: {项}\")\n",
    "\n",
    "        # 3. 取出加分项分数\n",
    "            # 如果是列表，则取平均值\n",
    "        if isinstance(对应列, list):\n",
    "            值列表 = [row.get(col, 0) for col in 对应列]\n",
    "            分项平均 = sum(值列表) / len(值列表) if 值列表 else 0\n",
    "        else:\n",
    "            分项平均 = row.get(对应列, 0)\n",
    "\n",
    "        分数 += 权重 * 分项平均\n",
    "\n",
    "    return 分数\n",
    "\n",
    "df_打分['加分原始得分'] = df_打分.apply(计算加分原始得分, axis=1)\n",
    "min_score = df_打分['加分原始得分'].min()\n",
    "max_score = df_打分['加分原始得分'].max()\n",
    "\n",
    "def 映射到065(x):\n",
    "    if pd.isna(x): return 0\n",
    "    if max_score == min_score:\n",
    "        return 65\n",
    "    return int(round(((x - min_score) / (max_score - min_score) * 65), 0 ))\n",
    "\n",
    "df_打分['加分得分'] = df_打分['加分原始得分'].apply(映射到065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_打分['总分'] = df_打分['加分得分'] + df_打分['底线匹配系数'] * 基本分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置支持中文字体（防止中文乱码）\n",
    "matplotlib.rcParams['font.family'] = 'SimHei'  # 或 'Microsoft YaHei', 视你系统而定\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False  # 负号也能显示\n",
    "\n",
    "# 画图区域设置\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 子图1：加分得分\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_打分['加分得分'].dropna(), bins=10, edgecolor='black', color='skyblue')\n",
    "plt.title('加分得分分布')\n",
    "plt.xlabel('加分得分')\n",
    "plt.ylabel('对数')\n",
    "\n",
    "# 子图2：综合总分\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_打分['总分'].dropna(), bins=10, edgecolor='black', color='salmon')\n",
    "plt.title('总分分布')\n",
    "plt.xlabel('总分')\n",
    "plt.ylabel('对数')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_统计 = df_打分.copy()\n",
    "print(df_统计.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 匹配成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有效问卷中，根据总分排序\n",
    "df_统计 = df_统计.sort_values(by='总分', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 限制匹配成功行数函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 限制匹配成功行数(\n",
    "    匹配成功数据: pd.DataFrame, \n",
    "    limit: int,\n",
    "    col_id_x: str = 'id_x',\n",
    "    col_id_y: str = 'id_y',\n",
    "    col_score: str = 'score'\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # 按组大小和分数同时排序\n",
    "    匹配成功组大小排序 = 匹配成功数据[col_id_x].value_counts()\n",
    "    # 添加辅助列：组大小\n",
    "    匹配成功数据['组大小'] = 匹配成功数据[col_id_x].map(匹配成功组大小排序)\n",
    "\n",
    "    排序后处理前数据 = 匹配成功数据.sort_values(\n",
    "        by=['组大小', col_id_x, col_score],  \n",
    "        ascending=[False, True, False]  \n",
    "    )\n",
    "    排序后处理前数据.drop(columns=['组大小'], inplace=True)\n",
    "    限行后数据 = 排序后处理前数据.copy()\n",
    "    \n",
    "    # 初步筛选出超限的组\n",
    "    超限组动态id池 = [\n",
    "        group_id for group_id, group_df in 限行后数据.groupby(col_id_x, observed=True) \n",
    "        if len(group_df) > limit\n",
    "    ]\n",
    "    超限组原始id池 = 超限组动态id池.copy()\n",
    "    # 创建一个空的数据表，用于存储删除数据\n",
    "    删除数据 = pd.DataFrame(columns=限行后数据.columns)\n",
    "\n",
    "    while 超限组动态id池:\n",
    "        # 取出第一个超限组\n",
    "        group_id = 超限组动态id池[0]\n",
    "        group = 限行后数据[限行后数据[col_id_x] == group_id]\n",
    "\n",
    "        # 如果长度超限，删除多余的行和对应匹配的行\n",
    "        if len(group) > limit:\n",
    "            # 找到当前需要删除的行和对应匹配行（排序上在limit之后的行）\n",
    "            drop_rows = group.iloc[limit:]\n",
    "            # 验证是否存在空的 DataFrame\n",
    "            if drop_rows.empty:\n",
    "                raise ValueError(\"drop_rows 或 match_rows 为空 DataFrame，无法继续执行删除操作\")\n",
    "            \n",
    "            drop_set = set([tuple(x) for x in drop_rows[[col_id_y, col_id_x]].to_numpy()])\n",
    "            match_rows = 限行后数据[\n",
    "                限行后数据.apply(\n",
    "                    lambda row: (row[col_id_x], row[col_id_y]) in drop_set \n",
    "                                or (row[col_id_y], row[col_id_x]) in drop_set,\n",
    "                    axis=1\n",
    "                )\n",
    "            ]\n",
    "            # 删除当前行和匹配行\n",
    "            to_drop = set(drop_rows.index).union(set(match_rows.index))  \n",
    "            限行后数据.drop(to_drop, inplace=True)\n",
    "            # 将删除的数据存储到删除数据表中\n",
    "            删除数据 = pd.concat(\n",
    "                [删除数据, drop_rows, match_rows],\n",
    "                ignore_index=True\n",
    "            ).astype(删除数据.dtypes)\n",
    "        else:\n",
    "            raise ValueError('超限组没有正确动态更新')\n",
    "\n",
    "        # 更新超限组列表，防止死循环\n",
    "        超限组动态id池 = [\n",
    "            group_id for group_id, group_df in 限行后数据.groupby(col_id_x, observed=True) \n",
    "            if len(group_df) > limit\n",
    "        ]\n",
    "    \n",
    "    # 初步筛选出低于限制的原超限组id\n",
    "    低限的原超限id_原始 = [\n",
    "        group_id for group_id in 超限组原始id池  \n",
    "        if len(限行后数据[限行后数据[col_id_x] == group_id]) < limit\n",
    "    ]\n",
    "    动态低限的原超限组id = 低限的原超限id_原始.copy()\n",
    "    \n",
    "    # 对数据进行排序\n",
    "    限行后数据组大小排序 = 限行后数据[col_id_x].value_counts()\n",
    "    限行后数据['组大小'] = 限行后数据[col_id_x].map(限行后数据组大小排序)\n",
    "    排序后限行后数据 = 限行后数据.sort_values(\n",
    "        by=['组大小', col_id_x, col_score],  \n",
    "        ascending=[False, True, False]  \n",
    "    )\n",
    "    排序后限行后数据.drop(columns=['组大小'], inplace=True)\n",
    "    限行后数据 = 排序后限行后数据.copy()\n",
    "\n",
    "    删除数据 = 删除数据.sort_values(by=[col_id_x, col_score], ascending=False)\n",
    "    \n",
    "    完成补充id = []\n",
    "    \n",
    "    # 补充低于限制的组\n",
    "    while 动态低限的原超限组id:\n",
    "        group_id = 动态低限的原超限组id[0]\n",
    "        \n",
    "        group_主表 = 限行后数据[限行后数据[col_id_x] == group_id]\n",
    "        待补充行数 = limit - len(group_主表)\n",
    "        \n",
    "        group_被删除 = 删除数据[删除数据[col_id_x] == group_id]\n",
    "\n",
    "        # 如果待补充行数大于0，且删除数据中存在对应的行\n",
    "        if 待补充行数 > 0 and len(group_被删除) > 0:\n",
    "            # 对 group_被删除 中的行依次尝试补充\n",
    "            for index, row in group_被删除.iterrows():\n",
    "                # 若此 row 对应的 col_id_y 在限行后数据里的同 col_id_y 行数也 < limit\n",
    "                if len(限行后数据[限行后数据[col_id_y] == row[col_id_y]]) < limit and 待补充行数 > 0:\n",
    "                    # 验证是否存在空的 DataFrame\n",
    "                    if row.empty:\n",
    "                        raise ValueError(\"drop_rows 或 match_rows 为空 DataFrame，无法继续执行删除操作\")\n",
    "                    # 将删除数据中的行和对应的匹配行添加到限行后数据中\n",
    "                    append_set = set([tuple(x) for x in row[[col_id_y, col_id_x]].to_numpy()])\n",
    "                    match_rows = 删除数据[\n",
    "                        删除数据.apply(\n",
    "                            lambda row: (row[col_id_x], row[col_id_y]) in append_set \n",
    "                                        or (row[col_id_y], row[col_id_x]) in append_set,\n",
    "                            axis=1\n",
    "                        )\n",
    "                    ]\n",
    "                    限行后数据 = pd.concat([限行后数据, match_rows], ignore_index=True)\n",
    "                    # 删除删除数据中的行\n",
    "                    删除数据.drop(index, inplace=True)\n",
    "                    待补充行数 -= 1\n",
    "                elif len(限行后数据[限行后数据[col_id_y] == row[col_id_y]]) >= limit and 待补充行数 > 0:\n",
    "                    # 如果同 col_id_y 已经达到了 limit，则不能补回主表，直接在删除表中移除\n",
    "                    删除数据.drop(index, inplace=True)\n",
    "                else:\n",
    "                    # 待补充行数耗尽则跳出\n",
    "                    break\n",
    "        \n",
    "        # 更新低限组列表\n",
    "        动态低限的原超限组id = [\n",
    "            gid for gid in 超限组原始id池\n",
    "            if len(限行后数据[限行后数据[col_id_x] == gid]) < limit\n",
    "        ]\n",
    "        完成补充id.append(group_id)\n",
    "        动态低限的原超限组id = [gid for gid in 动态低限的原超限组id if gid not in 完成补充id]\n",
    "        \n",
    "    # 最终对输出数据进行排序\n",
    "    限行后数据 = 限行后数据.sort_values(\n",
    "        by=[col_id_x, col_score], \n",
    "        ascending=[True, False]\n",
    "    )\n",
    "\n",
    "    return 限行后数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 是否匹配成功函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_统计.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 是否匹配成功(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 筛选总分 >= 60 的组合\n",
    "    匹配成功组合 = df[df['总分'] >= 60]\n",
    "    display(匹配成功组合.head())\n",
    "\n",
    "    # 创建反向 DataFrame，仅交换键列\n",
    "    df_反向 = 匹配成功组合.copy()\n",
    "    df_反向 = df_反向.rename(columns={\n",
    "        '识别用id_x': 'temp_id',\n",
    "        '填写平台_x': 'temp_platform',\n",
    "        '识别用id_y': '识别用id_x',\n",
    "        '填写平台_y': '填写平台_x'\n",
    "    })\n",
    "    df_反向['识别用id_y'] = df_反向['temp_id']\n",
    "    df_反向['填写平台_y'] = df_反向['temp_platform']\n",
    "    df_反向.drop(['temp_id', 'temp_platform'], axis=1, inplace=True)\n",
    "\n",
    "    # 设置索引进行匹配\n",
    "    匹配成功组合 = 匹配成功组合.set_index(['识别用id_x', '填写平台_x', '识别用id_y', '填写平台_y'])\n",
    "    df_反向 = df_反向.set_index(['识别用id_x', '填写平台_x', '识别用id_y', '填写平台_y'])\n",
    "\n",
    "    # 内连接：取交集的索引\n",
    "    共同索引 = 匹配成功组合.index.intersection(df_反向.index)\n",
    "    \n",
    "    display(df_反向.head())\n",
    "\n",
    "    # 从原始 DataFrame 中提取匹配行\n",
    "    df_匹配成功 = 匹配成功组合.loc[共同索引].reset_index()\n",
    "    \n",
    "    df_匹配成功 = 限制匹配成功行数(df_匹配成功, 5, '识别用id_x', '识别用id_y', '总分')\n",
    "    \n",
    "    return df_匹配成功\n",
    "\n",
    "df_匹配成功 = 是否匹配成功(df_统计)\n",
    "print(df_匹配成功.info())\n",
    "df_匹配成功 = df_匹配成功.copy()\n",
    "df_匹配成功['填写平台&识别用id'] = df_匹配成功['填写平台_x'] + df_匹配成功['识别用id_x']\n",
    "print(df_匹配成功.info())\n",
    "display(df_匹配成功.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计性输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收集到的问卷总数：df的行数\n",
    "问卷总数 = df.shape[0]\n",
    "# 收集到的有效问卷数：df_导入数据库的行数\n",
    "有效问卷数 = df_导入数据库.shape[0]\n",
    "# 无效问卷数 = 问卷总数 - 有效问卷数\n",
    "无效问卷数 = 问卷总数 - 有效问卷数\n",
    "# 有效问卷率 = 有效问卷数 / 问卷总数\n",
    "无效问卷率 = 1 - 有效问卷数 / 问卷总数\n",
    "# 匹配成功对数 = 总分大于60的行数 / 2\n",
    "匹配成功行数 = int(df_匹配成功.shape[0] / 2)\n",
    "# 填写平台&识别用id拼接后的字段不同的行数\n",
    "    # 给df_导入数据库添加填写平台&识别用id字段\n",
    "df_cleaned['填写平台&识别用id'] = df_cleaned['填写平台'] + df_cleaned['识别用id']\n",
    "匹配参加人数 = df_cleaned['填写平台&识别用id'].nunique()\n",
    "匹配成功人数 = df_匹配成功['填写平台&识别用id'].nunique()\n",
    "# 平均答题时长\n",
    "平均答题时长 = df_cleaned['答题时长'].mean()\n",
    "# 本周新增问卷数\n",
    "本周新增问卷数 = diff_df.shape[0]\n",
    "\n",
    "display(f'本轮无效问卷率为{无效问卷率:.2%}')\n",
    "display(f'匹配成功对数为{匹配成功行数}对，匹配参加人数为{匹配参加人数}人，匹配成功人数为{匹配成功人数}人，成功率为{匹配成功人数 / 匹配参加人数:.2%}')\n",
    "# 平均答题时长为几分几秒\n",
    "display(f'平均答题时长为{int(平均答题时长 // 60)}分{int(平均答题时长 % 60)}秒')\n",
    "# 删除填写平台&识别用id字段\n",
    "df_cleaned.drop(columns='填写平台&识别用id', inplace=True)\n",
    "\n",
    "display(pd.DataFrame(筛除结果))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置底线项和加分项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 统计影响因素(df: pd.DataFrame, 前缀: str) -> Dict[str, int]:\n",
    "    影响因素: Dict[str, int] = {}\n",
    "    for column in df.columns:\n",
    "        if column.startswith(前缀):\n",
    "            if 前缀 == '底线：':\n",
    "                影响因素[column] = df[column].sum()\n",
    "            elif 前缀 == '加分项：':\n",
    "                影响因素[column] = (df[column] == 0).sum()\n",
    "    return 影响因素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示底线项\n",
    "print(f\"总共行数：{df_统计.shape[0]}（有效问卷数*有效问卷数-有效问卷数）；不匹配的行数：\")\n",
    "\n",
    "# 调用函数统计底线项\n",
    "底线项 = 统计影响因素(df_统计, '底线：')\n",
    "display(底线项)\n",
    "\n",
    "# 调用函数统计加分项\n",
    "加分项 = 统计影响因素(df_统计, '加分项：')\n",
    "display(加分项)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 影响因素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计用列，专门用于评判除性取向和距离外哪些因素造成影响最大\n",
    "距离用列 = ['底线：家乡距离', '底线：常驻距离']\n",
    "# 筛选出统计用列数据都等于0的行\n",
    "df_影响因素 = df_打分[df_打分[距离用列].sum(axis=1) == 0]\n",
    "\n",
    "# 显示底线项\n",
    "print(f\"总共行数：{df_影响因素.shape[0]}（有效问卷数*有效问卷数-有效问卷数）；不匹配的行数：\")\n",
    "\n",
    "# 调用函数统计底线项\n",
    "影响因素_底线 = 统计影响因素(df_影响因素, '底线：')\n",
    "display(影响因素_底线)\n",
    "\n",
    "# 调用函数统计加分项\n",
    "影响因素_加分项 = 统计影响因素(df_影响因素, '加分项：')\n",
    "display(影响因素_加分项)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查询指定id的问卷是否有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入id，打印出该id是否存在于df_cleaned中和df1中\n",
    "def check_id(id: str) -> None:\n",
    "    # 判断id是否在df_cleaned中\n",
    "    if id in df_cleaned['识别用id'].values:\n",
    "        display(f'id为{id}的用户问卷有效')\n",
    "        display(df_匹配成功[df_匹配成功['识别用id_x'] == id])\n",
    "    else:\n",
    "        # 判断id是否在df1中\n",
    "        if id in df_cleaned['识别用id'].values:\n",
    "            display(f'收集到了id为{id}的问卷，但是无效')\n",
    "        else:\n",
    "            display(f'压根没收集到，或者问卷无效')\n",
    "            \n",
    "check_id('kiFte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查询嵌套占比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库连接信息\n",
    "user = 'postgres'\n",
    "password = 'root2'\n",
    "host = 'localhost'\n",
    "port = 5655 # 替换为你的数据库端口号\n",
    "database = 'postgres'  # 替换为你的数据库名称\n",
    "schema = '赛博相亲'  # 替换为你的架构名称\n",
    "table_name = '一次性相亲查询'  # 替换为你的表名称\n",
    "\n",
    "# 创建 PostgreSQL 数据库连接\n",
    "try:\n",
    "    conn = psycopg2.connect(database=database, user=user, password=password, host=host, port=port)\n",
    "    conn.autocommit = True  # 如果需要创建数据库，保持自动提交\n",
    "\n",
    "    # 设置搜索路径\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(f'SET search_path TO {schema}')\n",
    "    \n",
    "    print(\"连接成功并已设置搜索路径\")\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"数据库连接失败: {e}\")\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n",
    "\n",
    "# 创建 postgresql 数据库连接\n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}?options=-csearch_path={schema}')\n",
    "\n",
    "# 将数据存入数据库\n",
    "try:\n",
    "    df_cleaned.to_sql(table_name, engine, if_exists='replace', index=False, schema=schema)\n",
    "    print(\"数据成功插入数据库\")\n",
    "except Exception as e:\n",
    "    print(f\"数据插入失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询语句（可以考虑补个关于 订单期限的设置，排除npc订单，也就是大于100天的单子）\n",
    "query = \"\"\"\n",
    "select\n",
    "    \"工作性质\",\n",
    "    COUNT(*) AS 数量统计,  -- 统计数量\n",
    "    COUNT(*) * 1.0 / SUM(COUNT(*)) OVER () AS 占比  -- 计算占比\n",
    "from\n",
    "    一次性相亲查询  -- 数据来源表\n",
    "group by\n",
    "    \"工作性质\"  -- 按此字段分组\n",
    "order by\n",
    "    \"工作性质\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 将查询结果加载到 pandas 数据框中\n",
    "查询结果 = pd.read_sql(query, engine)\n",
    "\n",
    "# 打印数据框\n",
    "查询结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 指定id，指定项数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入id，输入列名，输出该id的该列名的数据\n",
    "def 输出指定id的指定数据(df: DataFrame, 填写平台: str, id: str, column: str) -> None:\n",
    "    # 判断id、填写平台、column是否在df_cleaned中，如果不在则直接退出循环并输出\n",
    "    if 填写平台 not in df['填写平台'].values:\n",
    "        display(f\"没有{填写平台}这个填写平台\")\n",
    "        return\n",
    "    if id not in df['识别用id'].values:\n",
    "        display(f\"没有id为{id}的用户\")\n",
    "        return\n",
    "    if column not in df.columns:\n",
    "        display(f\"没有{column}这一列\")\n",
    "        return\n",
    "    \n",
    "    display(f\"{填写平台}中{id}的{column}填写为{df[df['识别用id'] == id][column].values[0]}\")\n",
    "    \n",
    "# 填平台的适合需要加序号\n",
    "输出指定id的指定数据(df_cleaned,'A.小红书','502411749', 'TA的地址与你的距离（单位km）:常驻地址相距-底线')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输入id，输出底线项和筛除人数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入id，输出底线项和筛除人数\n",
    "def 输出底线项和筛除人数(df: DataFrame, 填写平台: str, id: str, 底线项: dict, 加分项: dict) -> None:\n",
    "    # 筛选出所有id为指定id，填写平台为指定填写平台的数据\n",
    "    df_id = df[(df['识别用id_x'] == id) & (df['填写平台_x'] == 填写平台)]\n",
    "    \n",
    "    # 计算总人选数\n",
    "    print(f\"总人选数：{df_id.shape[0]}\")\n",
    "    \n",
    "    # 统计底线项，底线项不为0的即为筛除人数\n",
    "    底线项_id = {key: (df_id[key] != 0).sum() for key in 底线项.keys()}\n",
    "    \n",
    "    # 统计加分项，加分项为0的即为筛除人数\n",
    "    加分项_id = {key: (df_id[key] <= 0).sum() for key in 加分项.keys()}\n",
    "    \n",
    "    # 输出id\n",
    "    print(f\"填写平台：{填写平台}，识别用id: {id}\")\n",
    "    # 输出底线项和加分项\n",
    "    print(\"底线项筛除人数统计：\")\n",
    "    for key, value in 底线项_id.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"\\n加分项筛除人数统计：\")\n",
    "    for key, value in 加分项_id.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "\n",
    "# 调用示例\n",
    "\"\"\"\n",
    "'A.小红书', '识别用id'] = df['小红书号id']\n",
    "'B.B站', '识别用id'] = df['B站昵称']\n",
    "'C.抖音', '识别用id'] = df['抖音号']\n",
    "'D.西瓜视频', '识别用id'] = df['抖音号']\n",
    "'E.博客（需填写邮箱）', '识别用id'] = df['公示用昵称'] + '-邮箱前四位为' + df['邮箱'].str[:4]\n",
    "'F.小黑盒', '识别用id'] = df['小黑盒id']\n",
    "\"\"\"\n",
    "\n",
    "def 看看为什么没匹配上 (填写平台: str, id: str, 底线项: dict, 加分项: dict) -> None:\n",
    "    \"\"\"key&value\n",
    "    底线：性取向: df_前置底线筛选[df_前置底线筛选['识别用id_x'] == id] - df_前置筛选_性取向通过[df_前置筛选_性取向通过['识别用id_x'] == id]\n",
    "    底线：是否打算在两年内结婚：df_前置筛选_性取向通过[df_前置筛选_性取向通过['识别用id_x'] == id] - df_前置筛选_结婚意愿通过[df_前置筛选_结婚意愿通过['识别用id_x'] == id]\n",
    "    底线：宗教信仰: df_前置筛选_结婚意愿通过[df_前置筛选_结婚意愿通过['识别用id_x'] == id] - df_前置筛选_宗教信仰通过[df_前置筛选_宗教信仰通过['识别用id_x'] == id]\n",
    "    \"\"\"\n",
    "    # 依序输出前置筛选中筛除的人数\n",
    "    print(\"（仅在前置筛选中是依序进行，第二项筛选的基数会比第一项小）\\n前置筛选情况：\")\n",
    "    print(f\"底线：性取向：{df_前置底线筛选[df_前置底线筛选['识别用id_x'] == id].shape[0] - df_前置筛选_性取向通过[df_前置筛选_性取向通过['识别用id_x'] == id].shape[0]}\")\n",
    "    print(f\"底线：是否打算在两年内结婚：{df_前置筛选_性取向通过[df_前置筛选_性取向通过['识别用id_x'] == id].shape[0] - df_前置筛选_结婚意愿通过[df_前置筛选_结婚意愿通过['识别用id_x'] == id].shape[0]}\")\n",
    "    print(f\"底线：宗教信仰：{df_前置筛选_结婚意愿通过[df_前置筛选_结婚意愿通过['识别用id_x'] == id].shape[0] - df_前置筛选_宗教信仰通过[df_前置筛选_宗教信仰通过['识别用id_x'] == id].shape[0]}\")\n",
    "    print(\"（后置筛选中，总人选的基数相同）\\n后置筛选情况：\")\n",
    "    输出底线项和筛除人数(df_统计, 填写平台, id, 底线项, 加分项)\n",
    "    print(\"\\n距离也匹配的人中：\")\n",
    "    输出底线项和筛除人数(df_影响因素, 填写平台,  id, 底线项, 加分项)\n",
    "\n",
    "# A.小红书\n",
    "# F.小黑盒\n",
    "看看为什么没匹配上('A.小红书', '1145171554', 底线项, 加分项)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输入id_x和id_y，输出TA对id_x的优缺点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入id_x和id_y，输出TA对id_x的优缺点（明文输出）\n",
    "def 输出TA对id_x的优缺点 (df: DataFrame, 填写平台_x: str, id_x: str, 填写平台_y: str, id_y: str) -> None:\n",
    "    # 筛选出所有id为指定id的行\n",
    "    df_id = df[(df['识别用id_x'] == id_x) & (df['识别用id_y'] == id_y)]\n",
    "    \n",
    "    # 检查是否找到符合条件的行\n",
    "    if df_id.empty:\n",
    "        print(f\"没有找到符合条件的行：id_x={id_x}, id_y={id_y}\")\n",
    "        return\n",
    "    \n",
    "    # 输出TA对id_x的优点，即底线项为0的数据的列名和加分项大于0的数据的列名\n",
    "    print(f\"TA对{id_x}的优点：\")\n",
    "    for column in df_id.columns:\n",
    "        if column.startswith('底线：') and df_id[column].values[0] == 0:\n",
    "            print(column)\n",
    "        \n",
    "    for column in df_id.columns:\n",
    "        if column.startswith('加分项：') and df_id[column].values[0] > 0:\n",
    "            print(column)\n",
    "    \n",
    "    # 输出TA对id_x的缺点，即底线项不为0的数据的列名和加分项等于0的数据的列名\n",
    "    print(f\"\\nTA对{id_x}的缺点：\")\n",
    "    for column in df_id.columns:\n",
    "        if column.startswith('底线：') and df_id[column].values[0] != 0:\n",
    "            print(column)\n",
    "    \n",
    "    for column in df_id.columns:\n",
    "        if column.startswith('加分项：') and df_id[column].values[0] == 0:\n",
    "            print(column)\n",
    "            \n",
    "输出TA对id_x的优缺点(df_统计,'B.B站','kiFte','A.小红书','9477396476')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 指定id，符合特定条件的匹配数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定id，符合特定条件的匹配数据\n",
    "df_特定条件 = df_统计[\n",
    "    (df_统计['填写平台_x'] == 'B.B站') &\n",
    "    (df_统计['识别用id_x'] == 'kiFte') &\n",
    "    (df_统计['底线：家庭总资产'].fillna(0) != 0)\n",
    "]\n",
    "\n",
    "# 定义匹配函数\n",
    "def 提取匹配数据(row):\n",
    "    底线成功 = [column.replace('底线：', '') for column in row.index if column.startswith('底线：') and row[column] == 0]\n",
    "    底线失败 = [column.replace('底线：', '') for column in row.index if column.startswith('底线：') and row[column] != 0]\n",
    "    优点 = [column.replace('加分项：', '') for column in row.index if column.startswith('加分项：') and row[column] > 0]\n",
    "    缺点 = [column.replace('加分项：', '') for column in row.index if column.startswith('加分项：') and row[column] == 0]\n",
    "    \n",
    "    return pd.Series({\n",
    "        'id_x': row['识别用id_x'],\n",
    "        'id_y': row['识别用id_y'],\n",
    "        '底线成功': ', '.join(底线成功),\n",
    "        '底线失败': ', '.join(底线失败),\n",
    "        '优点': ', '.join(优点),\n",
    "        '缺点': ', '.join(缺点)\n",
    "    })\n",
    "\n",
    "# 提取匹配数据\n",
    "df_特定条件 = df_特定条件.apply(提取匹配数据, axis=1)\n",
    "\n",
    "# 显示结果\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df_特定条件.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导出数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输出匹配成功数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 输出匹配成功数据(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    根据输入的 DataFrame 提取匹配成功数据，返回一个新 DataFrame\n",
    "    \"\"\"\n",
    "    # 创建一个列表用于收集行数据\n",
    "    rows = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # 创建一个字段记录他们的总分\n",
    "        总分 = row['总分']\n",
    "        # 提取优点和缺点，并移除列名中的 \"加分项：\"\n",
    "        优点 = [column.replace('加分项：', '') for column in df.columns if column.startswith('加分项：') and row[column] > 0]\n",
    "        缺点 = [column.replace('加分项：', '') for column in df.columns if column.startswith('加分项：') and row[column] <= 0]\n",
    "        \n",
    "        # 收集数据为字典\n",
    "        rows.append({\n",
    "            '填写平台_x': re.sub(r'^[A-Z]+\\.', '', row['填写平台_x']),  # 去掉任意字母加点的前缀\n",
    "            'id_x': row['识别用id_x'],\n",
    "            '填写平台_y': re.sub(r'^[A-Z]+\\.', '', row['填写平台_y']),  # 去掉任意字母加点的前缀\n",
    "            'id_y': row['识别用id_y'],\n",
    "            '总分': 总分,\n",
    "            '优点': ', '.join(优点),  # 将列表转换为字符串\n",
    "            '缺点': ', '.join(缺点)   # 将列表转换为字符串\n",
    "        })\n",
    "        \n",
    "        # 字典内部，根据填写平台_x、id_x、总分降序排序\n",
    "        rows = sorted(rows, key=lambda x: (x['填写平台_x'], x['id_x'], x['总分']), reverse=True)\n",
    "    \n",
    "    # 使用 pd.DataFrame 构造最终 DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "df_输出内容 = 输出匹配成功数据(df_匹配成功)\n",
    "# 使用 option_context 临时设置显示选项\n",
    "with pd.option_context(\n",
    "    'display.max_colwidth', None,  # 单元格内字符显示完整\n",
    "    'display.max_rows', None,      # 显示所有行\n",
    "    'display.max_columns', None,   # 显示所有列\n",
    "    'display.width', 0             # 自动适应宽度\n",
    "):\n",
    "    display(df_输出内容.head(4))  # 显示前4行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 匹配过期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库连接信息\n",
    "user = 'postgres'\n",
    "password = 'root2'\n",
    "host = 'localhost'\n",
    "port = 5655\n",
    "database = 'postgres'  # 替换为你的数据库名称\n",
    "schema = '赛博相亲'  # 替换为你的架构名称\n",
    "table_name_过期 = '匹配过期记录'  # 替换为你的表名称\n",
    "\n",
    "# 创建 PostgreSQL 数据库连接\n",
    "try:\n",
    "    conn = psycopg2.connect(database=database, user=user, password=password, host=host, port=port)\n",
    "    conn.autocommit = True  # 如果需要创建数据库，保持自动提交\n",
    "\n",
    "    # 设置搜索路径\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(f'SET search_path TO {schema}')\n",
    "    \n",
    "    print(\"连接成功并已设置搜索路径\")\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"数据库连接失败: {e}\")\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n",
    "\n",
    "# 创建 postgresql 数据库连接\n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}?options=-csearch_path={schema}')\n",
    "\n",
    "# 检查表是否存在\n",
    "with engine.begin() as conn:\n",
    "    check_table_query = f\"\"\"\n",
    "    SELECT EXISTS (\n",
    "        SELECT FROM information_schema.tables \n",
    "        WHERE table_schema = '{schema}' \n",
    "        AND table_name = '{table_name_过期}'\n",
    "    );\n",
    "    \"\"\"\n",
    "    table_exists = conn.execute(text(check_table_query)).scalar()\n",
    "    \n",
    "    extract_query = f\"\"\"\n",
    "    --- 处理两周过期的情况\n",
    "    with 上周计数 as (select id_x,\n",
    "                        count(*) as 计数\n",
    "                    from 相亲成功结果记录\n",
    "                    where EXTRACT(ISODOW FROM 日期) = 6\n",
    "                    and NOW()::date - 日期::date <= 7\n",
    "                    and NOW()::date - 日期::date > 0\n",
    "                    group by id_x\n",
    "                    having count(*) >= 5)\n",
    "\n",
    "    select\n",
    "        GREATEST(id_x,id_y) as id_a, --- 限制(x,y)的顺序，方便后续处理\n",
    "        LEAST(id_x,id_y) as id_b,\n",
    "        count(*) 计数\n",
    "    from 相亲成功结果记录\n",
    "    where EXTRACT(ISODOW FROM 日期) = 6\n",
    "    and NOW()::date - 日期::date <= 28\n",
    "    and id_x in (select id_x from 上周计数)\n",
    "    --- 确认数据并非已存在于匹配过期中\n",
    "    and not exists(\n",
    "            SELECT 1\n",
    "            FROM 匹配过期记录\n",
    "            where\n",
    "                相亲成功结果记录.id_x = 匹配过期记录.id_x\n",
    "                and 相亲成功结果记录.id_y = 匹配过期记录.id_y\n",
    "            )\n",
    "    group by id_a, id_b\n",
    "    having\n",
    "        count(*) >= 2\n",
    "\n",
    "    union\n",
    "\n",
    "    --- 处理四周过期的情况\n",
    "    select\n",
    "        GREATEST(id_x,id_y) as id_a,\n",
    "        LEAST(id_x,id_y) as id_b,\n",
    "        count(*) 计数\n",
    "    from 相亲成功结果记录\n",
    "    where EXTRACT(ISODOW FROM 日期) = 6\n",
    "    and NOW()::date - 日期::date <= 28\n",
    "    and not exists(\n",
    "        SELECT 1\n",
    "        FROM 匹配过期记录\n",
    "        where\n",
    "            相亲成功结果记录.id_x = 匹配过期记录.id_x\n",
    "            and 相亲成功结果记录.id_y = 匹配过期记录.id_y\n",
    "        )\n",
    "    group by id_a, id_b\n",
    "    having\n",
    "        count(*) >= 4;\n",
    "    \"\"\"\n",
    "\n",
    "    # 从数据库中提取数据\n",
    "    df_匹配过期 = pd.read_sql(extract_query, conn)\n",
    "    # 创建新列日期，插入当前日期\n",
    "    df_匹配过期['日期'] = pd.Timestamp.now().date()\n",
    "\n",
    "    # 根据表的存在性插入数据\n",
    "    if not table_exists:\n",
    "        # 如果表不存在，直接创建表并插入数据\n",
    "        print(f\"表 {schema}.{table_name_过期} 不存在，正在创建...\")\n",
    "        df_匹配过期.to_sql(table_name_过期, conn, schema=schema, if_exists='replace', index=False)\n",
    "        print(f\"表 {schema}.{table_name_过期} 创建并成功插入数据\")\n",
    "    else:\n",
    "        # 将df_匹配过期的 id_a,id_b 列重置为 id_x,id_y\n",
    "        df_匹配过期.rename(columns={'id_a': 'id_x', 'id_b': 'id_y'}, inplace=True)\n",
    "        df_匹配过期.to_sql(table_name_过期, conn, schema=schema, if_exists='append', index=False)\n",
    "        # 打印要导入的表原本有多少行，导入了多少行\n",
    "        print(f\"表 {schema}.{table_name_过期} 已存在，插入了 {df_匹配过期.shape[0]} 行数据\")\n",
    "        \n",
    "    # 取出匹配过期的全部数据\n",
    "    df_全匹配过期 = pd.read_sql(f\"SELECT * FROM {schema}.{table_name_过期}\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加无序配对的辅助列\n",
    "df_输出内容['id_a'] = df_输出内容[['id_x', 'id_y']].min(axis=1)\n",
    "df_输出内容['id_b'] = df_输出内容[['id_x', 'id_y']].max(axis=1)\n",
    "\n",
    "df_全匹配过期['id_a'] = df_全匹配过期[['id_x', 'id_y']].min(axis=1)\n",
    "df_全匹配过期['id_b'] = df_全匹配过期[['id_x', 'id_y']].max(axis=1)\n",
    "\n",
    "# 构建 set，提高效率\n",
    "过期组合集合 = set(zip(df_全匹配过期['id_a'], df_全匹配过期['id_b']))\n",
    "\n",
    "# 过滤未过期\n",
    "df_未过期 = df_输出内容[\n",
    "    ~df_输出内容.apply(lambda row: (row['id_a'], row['id_b']) in 过期组合集合, axis=1)\n",
    "].copy()\n",
    "\n",
    "# 删除辅助列\n",
    "df_未过期.drop(columns=['id_a', 'id_b'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小红书截图页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出列名\n",
    "df_未过期.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"优点代表id_y相对于id_x在这一加分项获取了分数，缺点代表在这一加分项没有获取分数\")\n",
    "print(\"图片中仅输出了'填写平台_x'为小红书的数据\")\n",
    "# 明确排除日期列，并筛选小红书数据\n",
    "df_小红书输出 = df_未过期.loc[\n",
    "    df_未过期['填写平台_x'] == '小红书',\n",
    "    df_未过期.columns.difference(['日期'])  # 移除日期列\n",
    "]\n",
    "with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "#with pd.option_context('display.max_colwidth', None):\n",
    "    # 生成HTML并隐藏索引\n",
    "    html_table = df_小红书输出.to_html(index=False)\n",
    "    display(HTML(html_table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检查：单向匹配&重复匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 单向匹配(df: DataFrame, id_x: str = '识别用id_x', id_y: str = '识别用id_y') -> None:\n",
    "    # 1. 创建一个反向 DataFrame，交换 id_x 和 id_y\n",
    "    df_反向 = df.rename(columns={id_x: id_y, id_y: id_x})\n",
    "\n",
    "    # 2. 使用 merge 查找哪些行在原表中没有对应的反向匹配\n",
    "    df_单向匹配 = df.merge(df_反向, on=[id_x, id_y], how='outer', indicator=True)\n",
    "\n",
    "    # 3. 筛选出仅在原表中存在，但不在反向表中的行\n",
    "    df_单向匹配 = df_单向匹配[df_单向匹配['_merge'] != 'both']\n",
    "    \n",
    "    # 同个id_x和id_y的行，出现多行，说明有重复匹配。仅保留重复次数大于1的部分\n",
    "    df_重复匹配 = df.groupby([id_x, id_y]).size().reset_index(name='重复次数')\n",
    "    df_重复匹配 = df_重复匹配[df_重复匹配['重复次数'] > 1]\n",
    "    \n",
    "    if df_单向匹配.empty and df_重复匹配.empty:\n",
    "        print(\"没有单向匹配和重复匹配的数据\")\n",
    "    else:\n",
    "        display(f\"存在单向匹配或重复匹配的数据，其中单向匹配的数据有 {df_单向匹配.shape[0]} 行，重复匹配的数据有 {df_重复匹配.shape[0]} 行\")\n",
    "        # 结果即为不成对的行\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "            print(\"单向匹配：\")\n",
    "            display(df_单向匹配.head())\n",
    "            print(\"重复匹配：\")\n",
    "            display(df_重复匹配.head())\n",
    "        # 报错退出\n",
    "        raise ValueError(\"存在单向匹配或重复匹配的数据\")\n",
    "\n",
    "单向匹配(df_匹配成功)\n",
    "单向匹配(df_未过期, 'id_x', 'id_y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输出导入数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给df_输出内容添加日期列\n",
    "df_未过期['日期'] = pd.to_datetime('today').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库连接信息\n",
    "user = 'postgres'\n",
    "password = 'root2'\n",
    "host = 'localhost'\n",
    "port = 5655\n",
    "database = 'postgres'  # 替换为你的数据库名称\n",
    "schema = '赛博相亲'  # 替换为你的架构名称\n",
    "table_name_1 = '相亲成功结果记录'  # 替换为你的表名称\n",
    "table_name_2 = '无效问卷记录v1_2'  # 替换为你的表名称\n",
    "\n",
    "# 创建 PostgreSQL 数据库连接\n",
    "try:\n",
    "    conn = psycopg2.connect(database=database, user=user, password=password, host=host, port=port)\n",
    "    conn.autocommit = True  # 如果需要创建数据库，保持自动提交\n",
    "\n",
    "    # 设置搜索路径\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(f'SET search_path TO {schema}')\n",
    "    \n",
    "    print(\"连接成功并已设置搜索路径\")\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"数据库连接失败: {e}\")\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n",
    "\n",
    "# 创建 postgresql 数据库连接\n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}?options=-csearch_path={schema}')\n",
    "\n",
    "# 检查表是否存在\n",
    "with engine.begin() as conn:\n",
    "    check_table_1_query = f\"\"\"\n",
    "    SELECT EXISTS (\n",
    "        SELECT FROM information_schema.tables \n",
    "        WHERE table_schema = '{schema}' \n",
    "        AND table_name = '{table_name_1}'\n",
    "    );\n",
    "    \"\"\"\n",
    "    check_table_2_query = f\"\"\"\n",
    "    SELECT EXISTS (\n",
    "        SELECT FROM information_schema.tables \n",
    "        WHERE table_schema = '{schema}' \n",
    "        AND table_name = '{table_name_2}'\n",
    "    );\n",
    "    \"\"\"   \n",
    "    table_1_exists = conn.execute(text(check_table_1_query)).scalar()\n",
    "    table_2_exists = conn.execute(text(check_table_2_query)).scalar()\n",
    "\n",
    "    # 根据表的存在性插入数据\n",
    "    if not table_1_exists:\n",
    "        # 如果表不存在，直接创建表并插入数据\n",
    "        print(f\"表 {schema}.{table_name_1} 不存在，正在创建...\")\n",
    "        df_未过期.to_sql(table_name_1, conn, schema=schema, if_exists='replace', index=False)\n",
    "        print(f\"表 {schema}.{table_name_1} 创建并成功插入数据\")\n",
    "    if not table_2_exists:\n",
    "        # 如果表不存在，直接创建表并插入数据\n",
    "        print(f\"表 {schema}.{table_name_2} 不存在，正在创建...\")\n",
    "        df_无效名单.to_sql(table_name_2, conn, schema=schema, if_exists='replace', index=False)\n",
    "        print(f\"表 {schema}.{table_name_2} 创建并成功插入数据\")\n",
    "\n",
    "    # 因为懒得改代码，故无论有没有插入过都跑一遍，避免插入完全重复的数据\n",
    "    print(f\"正在检查重复记录并尝试插入...\")\n",
    "    \n",
    "    # 获取数据库表的列名\n",
    "    columns_query_1 = f\"\"\"\n",
    "    SELECT column_name \n",
    "    FROM information_schema.columns \n",
    "    WHERE table_schema = '{schema}' AND table_name = '{table_name_1}'\n",
    "    ORDER BY ordinal_position;\n",
    "    \"\"\"\n",
    "    columns_query_2 = f\"\"\"\n",
    "    SELECT column_name\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = '{schema}' AND table_name = '{table_name_2}'\n",
    "    ORDER BY ordinal_position;\n",
    "    \"\"\"\n",
    "    # 读取列名\n",
    "    columns_1 = pd.read_sql(columns_query_1, conn)['column_name'].tolist()\n",
    "    columns_2 = pd.read_sql(columns_query_2, conn)['column_name'].tolist()\n",
    "    # 给所有列名加上双引号，避免 SQL 解析错误\n",
    "    columns_str_1 = ', '.join([f'\"{col}\"' for col in columns_1])\n",
    "    columns_str_2 = ', '.join([f'\"{col}\"' for col in columns_2])\n",
    "    # 将目前的数据替换至数据库中的临时表\n",
    "    temp_table_name_1 = f\"{table_name_1}_temp\"\n",
    "    temp_table_name_2 = f\"{table_name_2}_temp\"\n",
    "    df_未过期.to_sql(temp_table_name_1, conn, schema=schema, if_exists='replace', index=False)\n",
    "    df_无效名单.to_sql(temp_table_name_2, conn, schema=schema, if_exists='replace', index=False)\n",
    "    # 直接使用sql进行操作，查询临时表与数据库表的差集\n",
    "    diff_query_1 = f\"\"\"\n",
    "    SELECT {columns_str_1} FROM {schema}.{temp_table_name_1}\n",
    "    EXCEPT\n",
    "    SELECT {columns_str_1} FROM {schema}.{table_name_1};\n",
    "    \"\"\"\n",
    "    diff_query_2 = f\"\"\"\n",
    "    SELECT {columns_str_2} FROM {schema}.{temp_table_name_2}\n",
    "    EXCEPT\n",
    "    SELECT {columns_str_2} FROM {schema}.{table_name_2};\n",
    "    \"\"\"\n",
    "    diff_df_1 = pd.read_sql(diff_query_1, conn)\n",
    "    diff_df_2 = pd.read_sql(diff_query_2, conn)\n",
    "    # 如果有差集，插入差集数据\n",
    "    diff_df_1.to_sql(table_name_1, conn, schema=schema, if_exists='append', index=False)\n",
    "    diff_df_2.to_sql(table_name_2, conn, schema=schema, if_exists='append', index=False)\n",
    "    # 打印要导入的表原本有多少行，导入了多少行\n",
    "    print(f\"表 {schema}.{table_name_1} 原有 {df_未过期.shape[0]} 行数据，导入了 {diff_df_1.shape[0]} 行数据\")\n",
    "    print(f\"表 {schema}.{table_name_2} 原有 {df_无效名单.shape[0]} 行数据，导入了 {diff_df_2.shape[0]} 行数据\")\n",
    "    # 如果临时表存在，删除临时表\n",
    "    drop_temp_table_query = f\"\"\"\n",
    "    DROP TABLE IF EXISTS {schema}.{temp_table_name_1};\n",
    "    DROP TABLE IF EXISTS {schema}.{temp_table_name_2};\n",
    "    \"\"\"\n",
    "    # 删除临时表\n",
    "    conn.execute(text(drop_temp_table_query))\n",
    "    # 取出输出用无效名单\n",
    "    输出无效名单query = f\"\"\"\n",
    "    select\n",
    "        填写平台,\n",
    "        识别用id\n",
    "    from\n",
    "        {table_name_2}\n",
    "    where\n",
    "        日期 = current_date\n",
    "    \"\"\"\n",
    "    输出无效名单 = pd.read_sql(输出无效名单query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出路径\n",
    "导出路径1 = r\"D:\\code\\相亲问卷数据存储\\当前成功数据.xlsx\"\n",
    "导出路径2 = r\"D:\\code\\相亲问卷数据存储\\当前无效名单.xlsx\"\n",
    "# 导出匹配成功的数据，文件名重复会自动覆盖\n",
    "df_未过期.to_excel(导出路径1, index=False)\n",
    "输出无效名单.to_excel(导出路径2, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"无效原因主要以距离底线填为0和个人资产<家庭资产为主\")\n",
    "with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "#with pd.option_context('display.max_colwidth', None):\n",
    "    # 生成HTML并隐藏索引\n",
    "    html_table = 输出无效名单.to_html(index=False)\n",
    "    display(HTML(html_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反馈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_反馈 = df_cleaned.copy()\n",
    "df_反馈.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示完整列名\n",
    "with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "    display(df_反馈.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 个人总资产查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义分级范围和标签\n",
    "bins = [-float('inf'), 0, 10, 50, 100, 500, 1000, 3000, 8000, 15000, float('inf')]\n",
    "labels = ['0', '1-10', '10-50', '50-100', '100-500', '500-1000', '1000-3000', '3000-8000', '8000-15000', '15000+']\n",
    "\n",
    "# 定义需要分级的列及目标列名\n",
    "个人资产分级列 = {\n",
    "    '个人总资产（单位：万元）': '个人总资产分级',\n",
    "    'TA的个人总资产（单位：万元）_填空1': 'TA的个人总资产_底线分级',\n",
    "    'TA的个人总资产（单位：万元）_填空2': 'TA的个人总资产_加分项分级'\n",
    "}\n",
    "\n",
    "# 批量分级\n",
    "for source_col, target_col in 个人资产分级列.items():\n",
    "    df_反馈[target_col] = pd.cut(df_反馈[source_col], bins=bins, labels=labels, right=False)\n",
    "    # 手动处理 0 的情况\n",
    "    df_反馈.loc[df_反馈[source_col] == 0, target_col] = '0'\n",
    "\n",
    "# 构建包含所有列统计结果的表\n",
    "个人资产分级结果 = []\n",
    "for col_name in 个人资产分级列.values():\n",
    "    counts = df_反馈[col_name].value_counts()\n",
    "    percentages = df_反馈[col_name].value_counts(normalize=True) * 100\n",
    "    temp_df = pd.DataFrame({'分级列': col_name, '等级': counts.index, '行数': counts.values, '占比 (%)': percentages.values})\n",
    "    个人资产分级结果.append(temp_df)\n",
    "\n",
    "# 合并所有结果\n",
    "个人资产分级表 = pd.concat(个人资产分级结果, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制折线图\n",
    "个人资产分级图 = alt.Chart(个人资产分级表).mark_line(point=True).encode(\n",
    "    x=alt.X('等级', sort=labels, title='资产分级'),\n",
    "    y=alt.Y('行数', title='人数'),\n",
    "    color=alt.Color('分级列', title='资产类别'),\n",
    "    tooltip=['分级列', '等级', '行数', '占比 (%)']\n",
    ").properties(\n",
    "    title='个人资产分级趋势图',\n",
    "    width=800,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "# 展示图表\n",
    "个人资产分级图.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 家庭总资产查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义分级范围和标签\n",
    "bins = [-float('inf'), 0, 10, 50, 100, 500, 1000, 3000, 8000, 15000, float('inf')]\n",
    "labels = ['0', '1-10', '10-50', '50-100', '100-500', '500-1000', '1000-3000', '3000-8000', '8000-15000', '15000+']\n",
    "\n",
    "# 定义需要分级的列及目标列名\n",
    "家庭资产分级列 = {\n",
    "    '家庭总资产（单位：万元）': '家庭总资产分级',\n",
    "    'TA的家庭总资产（单位：万元）_填空1': 'TA的家庭总资产_底线分级',\n",
    "    'TA的家庭总资产（单位：万元）_填空2': 'TA的家庭总资产_加分项分级'\n",
    "}\n",
    "\n",
    "# 批量分级\n",
    "for source_col, target_col in 家庭资产分级列.items():\n",
    "    df_反馈[target_col] = pd.cut(df_反馈[source_col], bins=bins, labels=labels, right=False)\n",
    "    # 手动处理 0 的情况\n",
    "    df_反馈.loc[df_反馈[source_col] == 0, target_col] = '0'\n",
    "\n",
    "# 构建包含所有列统计结果的表\n",
    "家庭资产分级结果 = []\n",
    "for col_name in 家庭资产分级列.values():\n",
    "    counts = df_反馈[col_name].value_counts()\n",
    "    percentages = df_反馈[col_name].value_counts(normalize=True) * 100\n",
    "    temp_df = pd.DataFrame({'分级列': col_name, '等级': counts.index, '行数': counts.values, '占比 (%)': percentages.values})\n",
    "    家庭资产分级结果.append(temp_df)\n",
    "\n",
    "# 合并所有结果\n",
    "家庭资产分级表 = pd.concat(家庭资产分级结果, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制折线图\n",
    "家庭资产分级图 = alt.Chart(家庭资产分级表).mark_line(point=True).encode(\n",
    "    x=alt.X('等级', sort=labels, title='资产分级'),\n",
    "    y=alt.Y('行数', title='人数'),\n",
    "    color=alt.Color('分级列', title='资产类别'),\n",
    "    tooltip=['分级列', '等级', '行数', '占比 (%)']\n",
    ").properties(\n",
    "    title='家庭资产分级图',\n",
    "    width=800,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "# 展示图表\n",
    "家庭资产分级图.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 兴趣爱好匹配情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **1. 计算匹配成功的比例**\n",
    "\n",
    "# 筛选出性取向等于 0 的行\n",
    "df_性取向匹配 = df_打分[df_打分['底线：性取向'] == 0]\n",
    "\n",
    "# **2. 提取交集中的兴趣爱好**\n",
    "交集兴趣爱好 = []\n",
    "for _, row in df_性取向匹配.iterrows():\n",
    "    x_set = set(filter(None, map(str.strip, row['兴趣爱好_x'].split('.'))))  # 清理空格和空值\n",
    "    y_set = set(filter(None, map(str.strip, row['兴趣爱好_y'].split('.'))))  # 清理空格和空值\n",
    "    intersection = x_set & y_set  # 取交集\n",
    "    交集兴趣爱好.extend(intersection)  # 添加到交集列表\n",
    "\n",
    "# **3. 统计每个兴趣爱好的行数和匹配成功比例**\n",
    "交集兴趣爱好统计 = pd.Series(交集兴趣爱好).value_counts()\n",
    "交集兴趣爱好统计比例 = (交集兴趣爱好统计 / len(df_性取向匹配)) * 100\n",
    "\n",
    "# **4. 构建结果 DataFrame**\n",
    "兴趣爱好匹配情况 = pd.DataFrame({\n",
    "    '兴趣爱好':交集兴趣爱好统计.index,\n",
    "    '匹配成功行数（参考）':交集兴趣爱好统计.values,\n",
    "    '匹配成功比例 (%)':交集兴趣爱好统计比例.values\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# **2. 计算总体出现的比例**\n",
    "\n",
    "# **1. 按行拆分每种兴趣爱好并清理空格**\n",
    "兴趣爱好_x列 = df_打分['兴趣爱好_x'].str.split('.', expand=True).applymap(lambda x: x.strip() if x else None)\n",
    "兴趣爱好_y列 = df_打分['兴趣爱好_y'].str.split('.', expand=True).applymap(lambda x: x.strip() if x else None)\n",
    "\n",
    "# **2. 合并并获取独立兴趣爱好**\n",
    "兴趣爱好_展开 = pd.concat([兴趣爱好_x列, 兴趣爱好_y列], axis=1).stack().reset_index(drop=True)\n",
    "\n",
    "# **3. 过滤无效数据（空字符串或 None）**\n",
    "兴趣爱好_展开 = 兴趣爱好_展开[兴趣爱好_展开.notnull() & (兴趣爱好_展开.str.strip() != '')]\n",
    "\n",
    "# **4. 统计兴趣爱好行数（基于人数）**\n",
    "兴趣爱好总统计 = 兴趣爱好_展开.value_counts()\n",
    "\n",
    "# **5. 计算每种兴趣爱好占总人数的比例**\n",
    "兴趣爱好总统计比例 = (兴趣爱好总统计 / len(df_打分)) * 100\n",
    "\n",
    "# **6. 构建结果 DataFrame**\n",
    "总计_兴趣爱好统计 = pd.DataFrame({\n",
    "    '兴趣爱好': 兴趣爱好总统计.index,\n",
    "    '总出现行数（参考）': 兴趣爱好总统计.values / 2,\n",
    "    '出现比例 (%)': 兴趣爱好总统计比例.values / 2  # 除以 2 是因为每行都有两个人\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# **3. 合并两种比例到一个表中**\n",
    "\n",
    "兴趣爱好合并 = pd.merge(总计_兴趣爱好统计, 兴趣爱好匹配情况, on='兴趣爱好', how='left').fillna(0)\n",
    "print(兴趣爱好合并)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **3. 生成依序自增的英文字母**\n",
    "def 生成自增英文字母(n):\n",
    "    letters = []\n",
    "    for i in range(1, n + 1):\n",
    "        s = \"\"\n",
    "        while i > 0:\n",
    "            i -= 1\n",
    "            s = chr(65 + (i % 26)) + s  # A=65\n",
    "            i //= 26\n",
    "        letters.append(s)\n",
    "    return letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取出\"兴趣爱好：\"后的文本，去除空格，去除空值，去除重复值。\n",
    "# 创建一个表，列1为兴趣爱好，列2为依序自增的英文字母（从A开始，超过24个就变成AA，AB...）\n",
    "# 直接对包含兴趣爱好的列进行操作即可\n",
    "\n",
    "# **1. 提取兴趣爱好并清理数据**\n",
    "兴趣爱好列表 = [col.split(':')[1].strip() for col in 包含兴趣爱好的列]  # 提取兴趣爱好并去除空格\n",
    "\n",
    "# **2. 去除重复值（如果有）**\n",
    "兴趣爱好列表确保唯一 = pd.Series(兴趣爱好列表).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "兴趣爱好字母 = 生成自增英文字母(len(兴趣爱好列表确保唯一))\n",
    "\n",
    "# **3. 创建最终结果表**\n",
    "兴趣爱好说明表 = pd.DataFrame({\n",
    "    '说明': 兴趣爱好列表确保唯一,\n",
    "    '兴趣爱好': 兴趣爱好字母\n",
    "})\n",
    "\n",
    "兴趣爱好说明表['兴趣爱好'] = 兴趣爱好说明表['兴趣爱好'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 兴趣爱好情况输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "兴趣爱好比例统计 = pd.merge(兴趣爱好合并, 兴趣爱好说明表, on='兴趣爱好', how='left').fillna(0)\n",
    "\n",
    "# 筛选匹配成功比例小于 2 的行，并显式创建副本\n",
    "无必要兴趣爱好 = 兴趣爱好比例统计[兴趣爱好比例统计['匹配成功比例 (%)'] < 2].copy()\n",
    "\n",
    "# 调整总出现行数为 int，成功比例和出现比例保留 2 位小数\n",
    "无必要兴趣爱好['总出现行数（参考）'] = 无必要兴趣爱好['总出现行数（参考）'].astype(int)\n",
    "无必要兴趣爱好['匹配成功比例 (%)'] = 无必要兴趣爱好['匹配成功比例 (%)'].round(2)\n",
    "无必要兴趣爱好['出现比例 (%)'] = 无必要兴趣爱好['出现比例 (%)'].round(2)\n",
    "\n",
    "# 调整列顺序\n",
    "无必要兴趣爱好 = 无必要兴趣爱好[['说明', '匹配成功比例 (%)', '匹配成功行数（参考）', '出现比例 (%)', '总出现行数（参考）', '兴趣爱好']]\n",
    "\n",
    "# 排序\n",
    "无必要兴趣爱好 = 无必要兴趣爱好.sort_values(by='匹配成功比例 (%)', ascending=True)\n",
    "# 打印结果\n",
    "display(无必要兴趣爱好)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保说明列为字符串类型\n",
    "兴趣爱好比例统计['说明'] = 兴趣爱好比例统计['说明'].astype(str)\n",
    "# 根据匹配成功比例 (%) 对数据排序\n",
    "兴趣爱好比例统计 = 兴趣爱好比例统计.sort_values(by='匹配成功比例 (%)', ascending=True)\n",
    "\n",
    "# 提取排序后的说明顺序\n",
    "兴趣爱好展示顺序 = list(兴趣爱好比例统计['说明'])\n",
    "\n",
    "# 转换数据为长格式以便 Altair 处理\n",
    "兴趣爱好比例统计_long = 兴趣爱好比例统计.melt(\n",
    "    id_vars=['兴趣爱好', '说明'], \n",
    "    value_vars=['出现比例 (%)', '匹配成功比例 (%)'], \n",
    "    var_name='类型', \n",
    "    value_name='比例'\n",
    ")\n",
    "\n",
    "# 绘制折线图\n",
    "兴趣爱好比例图 = alt.Chart(兴趣爱好比例统计_long).mark_line(point=True).encode(\n",
    "    x=alt.X('说明:N', sort=兴趣爱好展示顺序, title='兴趣爱好'),\n",
    "    y=alt.Y('比例:Q', title='比例 (%)'),\n",
    "    color=alt.Color('类型:N', title='比例类型'),\n",
    "    tooltip=['说明', '类型', '比例']\n",
    ").properties(\n",
    "    title='兴趣爱好比例图',\n",
    "    width=800,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "# 展示图表\n",
    "兴趣爱好比例图.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth不填则匿名，zh中文，mul多语种\n",
    "HanLP = HanLPClient('https://www.hanlp.com/api', auth='NzMxMkBiYnMuaGFubHAuY29tOldLSTl4SW5lQmUyS2NMaGs=', language='zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载停用词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词表\n",
    "def load_stopwords(filepath):\n",
    "    \"\"\"加载停用词表\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        stopwords = set([line.strip() for line in f.readlines()])\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 去除空格和符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 去除空格和符号(text):\n",
    "    # 去除所有标点符号和空白空格\n",
    "    text = re.sub( r'[^\\w\\s]' , '' , text) # 保留字母、数字和中文字符\n",
    "    text = re.sub( r'\\s+' , ' ' , text) # 多个空格替换为单个空格\n",
    "    return text.strip() # 去掉首尾空格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词并过滤停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词并过滤停用词\n",
    "def 分词并过滤停用词(text, stopwords):\n",
    "    \"\"\"分词并过滤停用词\"\"\"\n",
    "    words = jieba.lcut(text)  # 精确模式分词\n",
    "    filtered_words = [word for word in words if word not in stopwords and len(word) > 1]  # 过滤停用词和长度为1的词\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成词云\n",
    "def generate_wordcloud(word_list):\n",
    "    \"\"\"根据词频生成词云\"\"\"\n",
    "    # 将词汇列表转换为字符串\n",
    "    text = ' '.join(word_list)\n",
    "\n",
    "    # 设置词云参数\n",
    "    wordcloud = WordCloud(\n",
    "        font_path='simhei.ttf',  # 中文字体路径 (Windows 默认 simhei.ttf 或 macOS 可用 STHeiti.ttf)\n",
    "        width=800,\n",
    "        height=600,\n",
    "        background_color='white',\n",
    "        max_words=200,\n",
    "        max_font_size=100\n",
    "    ).generate(text)\n",
    "\n",
    "    # 显示词云\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提取关键短语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 使用Hanlp提取关键词(text_list: list) -> dict:\n",
    "    # 初始化变量\n",
    "    total_length = sum(len(item) for item in text_list)\n",
    "    result = {}  # 存储关键词结果\n",
    "    api调用次数 = 0  # 统计API调用次数\n",
    "    暂存文本长度 = 0  # 暂存文本长度\n",
    "    批次文本上限 = 1000  # 每次调用API的文本上限\n",
    "    暂存文本最小项 = 0  # 暂存文本最小项\n",
    "    拼接符号占用长度 = 3  # 每个文本项之间的拼接符号占用长度\n",
    "    api每分钟可调用次数 = 4  # 每分钟API调用次数\n",
    "\n",
    "    if total_length <= 批次文本上限:\n",
    "        # 如果总长度小于批次文本上限，直接合并字符串，调用api即可\n",
    "        text = ' '.join([f\"'{item}'\" for item in text_list])\n",
    "        result.update(HanLP.keyphrase_extraction(text, topk=100))\n",
    "    else:\n",
    "        for i in range(0,len(text_list)):\n",
    "            if 暂存文本长度 + len(text_list[i]) + 拼接符号占用长度 <= 批次文本上限 and i != len(text_list) - 1:\n",
    "                暂存文本长度 = len(text_list[i]) +拼接符号占用长度 + 暂存文本长度\n",
    "            else:\n",
    "                if i == len(text_list) - 1 and 暂存文本长度 + len(text_list[i]) + 拼接符号占用长度 <= 批次文本上限:\n",
    "                    暂存文本最大项 = i + 1 # 如果是最后一项，且总长度<=批次文本上限，直接全部处理完毕\n",
    "                else:\n",
    "                    暂存文本最大项 = i\n",
    "                    暂存文本长度 = len(text_list[i]) +拼接符号占用长度 # 重置暂存文本长度\n",
    "                    \n",
    "                text = ' '.join([f\"'{item}'\" for item in text_list[暂存文本最小项:暂存文本最大项]])\n",
    "                失败次数 = 0\n",
    "                while 失败次数 < 3:  # 允许每段数据最多重试3次\n",
    "                    try:\n",
    "                        result.update(HanLP.keyphrase_extraction(text, topk=100))\n",
    "                        print(f\"第{暂存文本最小项}-{暂存文本最大项 - 1}条数据提取成功\")\n",
    "                        暂存文本最小项 = i\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        失败次数 += 1\n",
    "                        print(f\"第{暂存文本最小项}-{暂存文本最大项 - 1}条数据提取失败，重试次数 {失败次数}/3，错误信息：{str(e)}\")\n",
    "                        # 如果失败次数达到3次，则抛出异常退出\n",
    "                        if 失败次数 == 3:\n",
    "                            raise RuntimeError(f\"第{暂存文本最小项}-{暂存文本最大项 - 1}条数据连续3次提取失败，程序终止。\")\n",
    "                        # 等待60秒后重试\n",
    "                        time.sleep(60)\n",
    "                \n",
    "                # 更新API调用次数\n",
    "                api调用次数 += 1\n",
    "                # 每调用2次API后等待60秒\n",
    "                if api调用次数 % api每分钟可调用次数 == 0:\n",
    "                    print(\"API调用次数已达{api每分钟可调用次数}次，等待60秒...\")\n",
    "                    time.sleep(60)\n",
    "                \n",
    "                if i == len(text_list) - 1 and 暂存文本最大项 == i + 1:\n",
    "                    break # 已经处理完毕，退出循环\n",
    "                elif i == len(text_list) - 1 and 暂存文本最大项 == i:\n",
    "                    # i是最后一项，且暂存文本最大项等于i，说明最后一项需要单独处理\n",
    "                    失败次数 = 0\n",
    "                    text = f\"'{text_list[i]}'\"\n",
    "                    while 失败次数 < 3:  # 允许每段数据最多重试3次\n",
    "                        try:\n",
    "                            result.update(HanLP.keyphrase_extraction(text, topk=100))\n",
    "                            print(f\"第{i}条数据提取成功\")\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            失败次数 += 1\n",
    "                            print(f\"第{i}条数据提取失败，重试次数 {失败次数}/3，错误信息：{str(e)}\")\n",
    "                            # 如果失败次数达到3次，则抛出异常退出\n",
    "                            if 失败次数 == 3:\n",
    "                                raise RuntimeError(f\"第{i}条数据连续3次提取失败，程序终止。\")\n",
    "                            # 等待60秒后重试\n",
    "                            time.sleep(60)\n",
    "                else:\n",
    "                    continue\n",
    "                        \n",
    "    return result\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前置处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newhobiess = df_cleaned[['兴趣爱好（补充说明）', '再描述一些自己的特征']]\n",
    "print(df_newhobiess.shape[0])\n",
    "# 去除重复行\n",
    "df_newhobiess = df_cleaned[['兴趣爱好（补充说明）', '再描述一些自己的特征']].drop_duplicates()\n",
    "\n",
    "# 去除空白项或缺失值\n",
    "df_newhobiess = df_newhobiess.replace(r'^\\s*$', float('NaN'), regex=True)\n",
    "df_newhobiess = df_newhobiess.dropna(how='all')\n",
    "print(df_newhobiess.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换空白字符串为空值，然后用空字符串替代 NaN\n",
    "df_newhobiess['再描述一些自己的特征'] = df_newhobiess['再描述一些自己的特征'].fillna('').astype(str)\n",
    "df_newhobiess['兴趣爱好（补充说明）'] = df_newhobiess['兴趣爱好（补充说明）'].fillna('').astype(str)\n",
    "\n",
    "## 我希望对这些数据进行分词，然后统计词频\n",
    "补充特征统计_0 = df_newhobiess['再描述一些自己的特征'].values.tolist()\n",
    "兴趣爱好分词前准备_0 = df_newhobiess['兴趣爱好（补充说明）'].values.tolist()\n",
    "# 去重\n",
    "补充特征统计_2 = list(set(补充特征统计_0))\n",
    "兴趣爱好分词前准备_2 = list(set(兴趣爱好分词前准备_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_newhobiess.head(2))\n",
    "display(补充特征统计_0[:5])\n",
    "display(兴趣爱好分词前准备_0[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把列表中文本合并成一个字符串\n",
    "补充特征统计_2_str = ' '.join(补充特征统计_2)\n",
    "兴趣爱好分词前准备_2_str = ' '.join(兴趣爱好分词前准备_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载挺有词典\n",
    "stopwords = load_stopwords(\"停用词典.txt\")\n",
    "# 加载自定义词典\n",
    "jieba.load_userdict(\"行为特征词典.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本预处理\n",
    "特征_关键短语文本 = 去除空格和符号(补充特征统计_2_str)\n",
    "兴趣爱好_关键短语文本 = 去除空格和符号(兴趣爱好分词前准备_2_str)\n",
    "display(特征_关键短语文本)\n",
    "display(兴趣爱好_关键短语文本)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动态调整词频\n",
    "jieba.suggest_freq(('想找'), True)  # 强制识别\"想找\"\n",
    "jieba.suggest_freq(('北方人'), True)  # 确保\"北方人\"不被拆分\n",
    "jieba.suggest_freq(('一个人'), True)  # 确保\"北方人\"不被拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "特征分词结果 = 分词并过滤停用词(特征_关键短语文本, stopwords)\n",
    "兴趣爱好分词结果 = 分词并过滤停用词(兴趣爱好_关键短语文本, stopwords)\n",
    "# 统计词频\n",
    "特征词频 = Counter(特征分词结果)\n",
    "兴趣词频 = Counter(兴趣爱好分词结果)\n",
    "# 生成词云\n",
    "display(特征分词结果[:5])\n",
    "print(\"特征词频前10：\", 特征词频.most_common(10))\n",
    "generate_wordcloud(特征分词结果)\n",
    "display(兴趣爱好分词结果[:5])\n",
    "print(\"兴趣词频前10：\", 兴趣词频.most_common(10))\n",
    "generate_wordcloud(兴趣爱好分词结果)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
